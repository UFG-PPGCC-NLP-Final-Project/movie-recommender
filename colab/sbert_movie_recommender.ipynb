{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UFG-PPGCC-NLP-Final-Project/movie-recommender/blob/main/colab/sbert_movie_recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emp9JbsOrzPx"
      },
      "source": [
        "# SBERT Movie Recommender System\n",
        "\n",
        "**Adapta√ß√£o do artigo**: *BERT one-shot movie recommender system* usando Sentence-BERT\n",
        "\n",
        "Este notebook implementa um sistema de recomenda√ß√£o de filmes end-to-end usando SBERT (Sentence-BERT), projetado para produzir recomenda√ß√µes estruturadas a partir de queries n√£o estruturadas.\n",
        "\n",
        "## ‚ö†Ô∏è PROBLEMAS IDENTIFICADOS E SOLU√á√ïES\n",
        "\n",
        "### üî¥ Problemas nos Experimentos 1-4 (Original Trainer)\n",
        "\n",
        "| Problema | Causa | Impacto |\n",
        "|----------|-------|---------|\n",
        "| **Stagna√ß√£o em nDCG@10=0.044** | `BCEWithLogitsLoss()` sem pesos + desbalanceamento 1:1200 | Modelo aprende a predizer ~0 para tudo |\n",
        "| **RNN sem efeito** (+0.0019 vs +0.035 esperado) | Loss inadequado impede aprender features colaborativas | RNN funciona, mas otimiza√ß√£o ignora |\n",
        "| **Multi-task quebrado** (Train Loss=6.2) | CE loss (‚âà6.0) domina BCE loss (‚âà0.003) | Modelo otimiza tags, ignora filmes |\n",
        "\n",
        "### ‚úÖ Solu√ß√µes Implementadas (Weighted Trainer - Se√ß√£o 7.1)\n",
        "\n",
        "1. **Weighted BCE Loss**: `pos_weight=1200` para balancear classes positivas/negativas\n",
        "2. **Multi-task balanceado**: `loss = bce_loss + (0.001 * ce_loss)` para equalizar contribui√ß√µes\n",
        "3. **Monitoramento detalhado**: Track BCE e CE losses separadamente\n",
        "\n",
        "**Expectativa**: nDCG@10 deve subir de **0.044 ‚Üí > 0.10** em 20 √©pocas.\n",
        "\n",
        "---\n",
        "\n",
        "## Arquitetura\n",
        "- **Baseline**: SBERT + FFN para classifica√ß√£o multi-label\n",
        "- **Extens√£o 1**: SBERT + RNN para features colaborativas\n",
        "- **Extens√£o 2**: Multi-task learning com dados de tags de usu√°rios\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9ml9dNSrzPz"
      },
      "source": [
        "## 1. Configura√ß√£o do Ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MRd55n-xrzPz"
      },
      "outputs": [],
      "source": [
        "# Verificar GPU dispon√≠vel\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ORJobH3LrzP0"
      },
      "outputs": [],
      "source": [
        "# Instalar depend√™ncias\n",
        "!pip install -q sentence-transformers transformers datasets torch accelerate scikit-learn pandas numpy tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JF2beBfgrzP0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import ndcg_score\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Configurar seeds para reprodutibilidade\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Configurar device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Usando device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Mem√≥ria total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDrse0LLrzP0"
      },
      "source": [
        "## 2. Configura√ß√µes e Hiperpar√¢metros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8K21S8x6rzP0"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    \"\"\"Configura√ß√µes do modelo e treinamento\"\"\"\n",
        "\n",
        "    # Modelo SBERT\n",
        "    sbert_model_name = 'sentence-transformers/all-MiniLM-L6-v2'  # Modelo SBERT eficiente\n",
        "    sbert_hidden_size = 384  # Dimens√£o do all-MiniLM-L6-v2\n",
        "\n",
        "    # RNN para features colaborativas\n",
        "    rnn_embedding_size = 256\n",
        "    rnn_hidden_size = 128\n",
        "\n",
        "    # FFN\n",
        "    ffn_hidden_size = 256\n",
        "    dropout_prob = 0.1  # Reduzido de 0.3 para 0.1 (dataset pequeno precisa menos regulariza√ß√£o)\n",
        "\n",
        "    # Treinamento\n",
        "    movies_batch_size = 32  # Aumentado para 32 (GPU tem capacidade, gradiente mais est√°vel)\n",
        "    tags_batch_size = 64    # Mantido (tarefa auxiliar simples)\n",
        "    learning_rate = 2e-5    # Aumentado de 1e-5 para 2e-5 (converg√™ncia mais r√°pida)\n",
        "    num_epochs = 20         # Reduzido para demonstra√ß√£o (artigo usa 200)\\n\",\n",
        "    warmup_ratio = 0.1\n",
        "    max_seq_length = 512\n",
        "\n",
        "    # Dataset\n",
        "    num_movies = None  # Ser√° definido automaticamente pelo movie_mapper.get_num_movies()\n",
        "\n",
        "    # Avalia√ß√£o\n",
        "    eval_k = 10            # nDCG@10\n",
        "\n",
        "    # Checkpoints\n",
        "    save_dir = './checkpoints'\n",
        "\n",
        "config = Config()\n",
        "os.makedirs(config.save_dir, exist_ok=True)\n",
        "\n",
        "# Imprimir configura√ß√µes importantes\n",
        "print(\"=\"*60)\n",
        "print(\"HIPERPAR√ÇMETROS CONFIGURADOS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Learning Rate: {config.learning_rate}\")\n",
        "print(f\"Dropout: {config.dropout_prob}\")\n",
        "print(f\"Movies Batch Size: {config.movies_batch_size}\")\n",
        "print(f\"Tags Batch Size: {config.tags_batch_size}\")\n",
        "print(f\"Num Epochs: {config.num_epochs}\")\n",
        "print(f\"Max Seq Length: {config.max_seq_length}\")\n",
        "print(f\"Num Movies: {config.num_movies} (ser√° definido automaticamente)\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Explica√ß√£o dos Hiperpar√¢metros Ajustados\n",
        "\n",
        "**Mudan√ßas em rela√ß√£o √† configura√ß√£o original para melhorar resultados:**\n",
        "\n",
        "#### 1. **Learning Rate: 1e-5 ‚Üí 2e-5** (2x maior)\n",
        "   - **O que faz:** Controla o tamanho dos \"passos\" ao atualizar pesos da rede neural\n",
        "   - **Por que mudar:** 1e-5 √© muito conservador para dataset pequeno\n",
        "   - **Impacto esperado:** Converg√™ncia mais r√°pida sem perder estabilidade\n",
        "   - **Alternativas:** Experimentar 5e-5 se 2e-5 n√£o melhorar\n",
        "\n",
        "#### 2. **Dropout: 0.3 ‚Üí 0.1** (3x menor)\n",
        "   - **O que faz:** \"Desliga\" aleatoriamente 10% dos neur√¥nios durante treino\n",
        "   - **Por que mudar:** Dataset pequeno (~9K exemplos) + 30% dropout = underfitting\n",
        "   - **Impacto esperado:** Modelo consegue aprender melhor os padr√µes\n",
        "   - **Trade-off:** Menos regulariza√ß√£o, mas dataset pequeno precisa de capacidade\n",
        "\n",
        "#### 3. **Movies Batch Size: 8 ‚Üí 16 ‚Üí 32** (4x maior)\n",
        "   - **O que faz:** N√∫mero de exemplos processados juntos antes de atualizar pesos\n",
        "   - **Por que 32:** GPU tem capacidade + tempo de execu√ß√£o melhorou\n",
        "   - **Vantagens:** Gradiente mais est√°vel, treinamento mais r√°pido, menos vari√¢ncia\n",
        "   - **Diferen√ßa de tags_batch_size:** \n",
        "     * movies (32): Tarefa complexa (di√°logos ‚Üí m√∫ltiplos filmes)\n",
        "     * tags (64): Tarefa simples (1 tag ‚Üí 1 filme)\n",
        "   - **Monitorar:** Se der \"CUDA out of memory\", reduzir para 16\n",
        "\n",
        "#### 4. **Num Epochs: 30 ‚Üí 50 ‚Üí 20** (ajustado para testes)\n",
        "   - **O que faz:** N√∫mero de vezes que todo dataset √© visto\n",
        "   - **Por que 20:** Testes iniciais antes de treino completo\n",
        "   - **Quando avaliar:**\n",
        "     * 8 √©pocas: Muito cedo para conclus√µes\n",
        "     * 20 √©pocas: Primeira avalia√ß√£o real\n",
        "     * 50+ √©pocas: Treinamento completo (artigo usa 200)\n",
        "   - **Dica:** Observe nDCG@10 entre √©pocas 10-20 para decidir continuar\n",
        "\n",
        "#### 5. **num_movies: 6924 ‚Üí None (autom√°tico)**\n",
        "   - **Problema original:** Valor hardcoded n√£o correspondia ao dataset real\n",
        "   - **Solu√ß√£o:** Calculado automaticamente via `movie_mapper.get_num_movies()`\n",
        "   - **Impacto:** Evita bugs e garante dimens√£o correta da camada de sa√≠da\n",
        "\n",
        "---\n",
        "\n",
        "### 2.2 Como Avaliar os Resultados\n",
        "\n",
        "**‚è±Ô∏è Cronograma de Avalia√ß√£o:**\n",
        "\n",
        "| √âpocas | O que esperar | O que observar |\n",
        "|--------|---------------|----------------|\n",
        "| 1-5    | Aprendizado b√°sico | Loss caindo rapidamente |\n",
        "| 5-10   | Primeiras melhorias | nDCG@10 come√ßando a subir (0.01 ‚Üí 0.03) |\n",
        "| 10-20  | **Avalia√ß√£o cr√≠tica** | Se nDCG@10 > 0.05, continuar. Se < 0.03, revisar |\n",
        "| 20-50  | Converg√™ncia real | nDCG@10 deve atingir 0.08-0.13 |\n",
        "| 50+    | Fine-tuning | Ganhos marginais, risco de overfitting |\n",
        "\n",
        "**üìä Sinais de Sucesso:**\n",
        "- ‚úÖ nDCG@10 crescendo consistentemente\n",
        "- ‚úÖ Train loss < Eval loss, mas gap pequeno (<0.01)\n",
        "- ‚úÖ Recall@10 acompanhando nDCG@10\n",
        "\n",
        "**‚ö†Ô∏è Sinais de Problema:**\n",
        "- ‚ùå nDCG@10 estagnado em < 0.03 ap√≥s 15 √©pocas\n",
        "- ‚ùå Eval loss > Train loss (underfitting) ‚Üí reduzir dropout ou aumentar LR\n",
        "- ‚ùå Train loss << Eval loss (overfitting) ‚Üí aumentar dropout ou reduzir LR\n",
        "\n",
        "**üéØ Meta Realista:**\n",
        "- Com SBERT/BERT + RNN + Multi-Task: **nDCG@10 ‚âà 0.10-0.15** em 20-50 √©pocas\n",
        "- Artigo original reporta: **0.130-0.169** com 200 √©pocas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Pu6zliCrzP1"
      },
      "source": [
        "## 3. Carregamento e Processamento dos Dados\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI3m0bgxTjSy"
      },
      "source": [
        "### 3.1 Dataset ReDial\n",
        "O dataset ReDial cont√©m di√°logos de recomenda√ß√£o de filmes entre um iniciador e um respondente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mvwZO-SEGIz"
      },
      "source": [
        "#### 3.1.1 Acesso ao dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5gmYyA8rzP1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Carregar dataset ReDial\n",
        "print(\"Carregando dataset ReDial...\")\n",
        "redial_dataset_raw = load_dataset('community-datasets/re_dial')\n",
        "print(f\"Train: {len(redial_dataset_raw['train'])} exemplos\")\n",
        "print(f\"Test: {len(redial_dataset_raw['test'])} exemplos\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Visualizar estrutura de um exemplo\n",
        "sample = redial_dataset_raw['train'][0]\n",
        "print(\"Estrutura de um exemplo:\")\n",
        "for key in sample.keys():\n",
        "    print(f\"  {key}: {type(sample[key])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V3QeabSrzP1"
      },
      "outputs": [],
      "source": [
        "class MovieIDMapper:\n",
        "    \"\"\"\n",
        "    Mapeia IDs de filmes entre diferentes formatos a partir do dataset ReDial. Exemplo: '@123' -> 123\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.movie_to_idx = {}\n",
        "        self.idx_to_movie = {}\n",
        "        self.movie_names = {}\n",
        "\n",
        "    def build_from_dataset(self, dataset):\n",
        "        \"\"\"Constr√≥i mapeamento a partir do dataset ReDial\"\"\"\n",
        "        all_movies = set()\n",
        "\n",
        "        for split in ['train', 'test']:\n",
        "            for original_content in dataset[split]:\n",
        "                # Extrair IDs de filmes das mensagens\n",
        "                messages = original_content.get('messages', [])\n",
        "                for msg in messages:\n",
        "                    text = msg.get('text', '')\n",
        "                    movie_ids = re.findall(r'@(\\d+)', text)\n",
        "                    all_movies.update(movie_ids)\n",
        "\n",
        "                # Extrair dos movieMentions\n",
        "                mentions = original_content.get('movieMentions', {})\n",
        "                if isinstance(mentions, dict):\n",
        "                    for movie_id, name in mentions.items():\n",
        "                        all_movies.add(str(movie_id).replace('@', ''))\n",
        "                        self.movie_names[str(movie_id).replace('@', '')] = name\n",
        "\n",
        "        # Criar mapeamento ordenado\n",
        "        sorted_movies = sorted([int(m) for m in all_movies if m.isdigit()])\n",
        "\n",
        "        for idx, movie_id in enumerate(sorted_movies):\n",
        "            self.movie_to_idx[str(movie_id)] = idx\n",
        "            self.idx_to_movie[idx] = str(movie_id)\n",
        "\n",
        "        print(f\"Total de filmes √∫nicos: {len(self.movie_to_idx)}\")\n",
        "        return self\n",
        "\n",
        "    def get_num_movies(self):\n",
        "        return len(self.movie_to_idx)\n",
        "\n",
        "    def movie_id_to_idx(self, movie_id):\n",
        "        movie_id = str(movie_id).replace('@', '')\n",
        "        return self.movie_to_idx.get(movie_id, -1)\n",
        "\n",
        "    def idx_to_movie_id(self, idx):\n",
        "        return self.idx_to_movie.get(idx, None)\n",
        "\n",
        "# Construir mapeamento\n",
        "movie_mapper = MovieIDMapper().build_from_dataset(redial_dataset_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brpc9myKrzP1"
      },
      "source": [
        "#### 3.1.2 Processamento dos Di√°logos\n",
        "\n",
        "Conforme o artigo, concatenamos as utterances do iniciador com tokens [SEP] e usamos as recomenda√ß√µes do respondente como labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yTMSjZArzP1"
      },
      "outputs": [],
      "source": [
        "def process_dialogue(example, movie_mapper):\n",
        "    \"\"\"\n",
        "    Processa um di√°logo do ReDial conforme descrito no artigo:\n",
        "    - Input: utterances do iniciador concatenadas com [SEP]\n",
        "    - Output: IDs dos filmes recomendados pelo respondente\n",
        "    - Movies mentioned: filmes mencionados pelo iniciador (para RNN)\n",
        "    \"\"\"\n",
        "    messages = example.get('messages', [])\n",
        "\n",
        "    initiator_texts = []\n",
        "    mentioned_movies = []  # Filmes mencionados pelo iniciador\n",
        "    recommended_movies = []  # Filmes recomendados pelo respondente\n",
        "\n",
        "    for msg in messages:\n",
        "        text = msg.get('text', '')\n",
        "        sender_id = msg.get('senderWorkerId', 0)\n",
        "\n",
        "        # Extrair IDs de filmes\n",
        "        movie_ids = re.findall(r'@(\\d+)', text)\n",
        "\n",
        "        # Determinar se √© iniciador (primeiro sender) ou respondente\n",
        "        if sender_id == messages[0].get('senderWorkerId', 0):\n",
        "            # Iniciador - adicionar texto e filmes mencionados\n",
        "            # Substituir IDs por placeholder para vers√£o sem RNN\n",
        "            clean_text = re.sub(r'@\\d+', '@', text)\n",
        "            initiator_texts.append(clean_text)\n",
        "            mentioned_movies.extend(movie_ids)\n",
        "        else:\n",
        "            # Respondente - coletar recomenda√ß√µes\n",
        "            recommended_movies.extend(movie_ids)\n",
        "\n",
        "    # Concatenar textos do iniciador com [SEP]\n",
        "    input_text = ' [SEP] '.join(initiator_texts)\n",
        "\n",
        "    # Converter IDs para √≠ndices\n",
        "    mentioned_indices = [movie_mapper.movie_id_to_idx(m) for m in mentioned_movies]\n",
        "    mentioned_indices = [idx for idx in mentioned_indices if idx >= 0]\n",
        "\n",
        "    recommended_indices = [movie_mapper.movie_id_to_idx(m) for m in recommended_movies]\n",
        "    recommended_indices = list(set([idx for idx in recommended_indices if idx >= 0]))\n",
        "\n",
        "    return {\n",
        "        'input_text': input_text,\n",
        "        'input_text_with_ids': ' [SEP] '.join([msg.get('text', '') for msg in messages\n",
        "                                               if msg.get('senderWorkerId') == messages[0].get('senderWorkerId')]),\n",
        "        'mentioned_movies': mentioned_indices,\n",
        "        'recommended_movies': recommended_indices\n",
        "    }\n",
        "\n",
        "# Processar dataset\n",
        "def process_split(dataset_split, movie_mapper):\n",
        "    processed = []\n",
        "    for example in tqdm(dataset_split, desc=\"Processando\"):\n",
        "        proc = process_dialogue(example, movie_mapper)\n",
        "        # Filtrar exemplos sem recomenda√ß√µes\n",
        "        if proc['recommended_movies'] and proc['input_text'].strip():\n",
        "            processed.append(proc)\n",
        "    return processed\n",
        "\n",
        "print(\"Processando split de treino...\")\n",
        "train_data = process_split(redial_dataset_raw['train'], movie_mapper)\n",
        "print(f\"Exemplos de treino v√°lidos: {len(train_data)}\")\n",
        "\n",
        "print(\"\\nProcessando split de teste...\")\n",
        "test_data = process_split(redial_dataset_raw['test'], movie_mapper)\n",
        "print(f\"Exemplos de teste v√°lidos: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jdiP4qAWrzP2"
      },
      "outputs": [],
      "source": [
        "# Visualizar exemplo processado\n",
        "print(\"Exemplo processado:\")\n",
        "print(f\"Input text: {train_data[0]['input_text'][:500]}...\")\n",
        "print(f\"\\nMentioned movies (√≠ndices): {train_data[0]['mentioned_movies'][:5]}\")\n",
        "print(f\"Recommended movies (√≠ndices): {train_data[0]['recommended_movies']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EOQYxyVrzP2"
      },
      "source": [
        "### 3.2 Dataset MovieLens\n",
        "\n",
        "Para o experimento de multi-task learning, usamos tags de usu√°rios do MovieLens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ-Adm7vrzP2"
      },
      "outputs": [],
      "source": [
        "# Download MovieLens tags\n",
        "!wget -q -nc http://files.grouplens.org/datasets/movielens/ml-latest.zip\n",
        "!unzip -q -o ml-latest.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PLyP7aa0rzP2"
      },
      "outputs": [],
      "source": [
        "# Carregar dados do MovieLens\n",
        "tags_csv = pd.read_csv('ml-latest/tags.csv')\n",
        "movies_csv = pd.read_csv('ml-latest/movies.csv')\n",
        "\n",
        "print(f\"Tags totais: {len(tags_csv)}\")\n",
        "print(f\"Filmes totais: {len(movies_csv)}\")\n",
        "print(f\"\\nExemplo de tags:\")\n",
        "print(tags_csv.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zbQ8_hmHrzP2"
      },
      "outputs": [],
      "source": [
        "def create_tag_dataset(tags_csv, movie_mapper, max_tags_per_movie=50):\n",
        "    \"\"\"\n",
        "    Cria dataset de tags para multi-task learning\n",
        "    Input: tag text\n",
        "    Output: movie index\n",
        "    \"\"\"\n",
        "    tag_data = []\n",
        "\n",
        "    # Agrupar tags por filme\n",
        "    for movie_id, group in tags_csv.groupby('movieId'):\n",
        "        movie_idx = movie_mapper.movie_id_to_idx(str(movie_id))\n",
        "        if movie_idx < 0:\n",
        "            continue\n",
        "\n",
        "        tags = group['tag'].tolist()[:max_tags_per_movie]\n",
        "        for tag in tags:\n",
        "            if isinstance(tag, str) and len(tag.strip()) > 2:\n",
        "                tag_data.append({\n",
        "                    'tag_text': tag.strip(),\n",
        "                    'movie_idx': movie_idx\n",
        "                })\n",
        "\n",
        "    return tag_data\n",
        "\n",
        "# Criar dataset de tags (movie_mapper definido junto do dataset do redial)\n",
        "tag_data = create_tag_dataset(tags_csv, movie_mapper)\n",
        "print(f\"Exemplos de tags: {len(tag_data)}\")\n",
        "print(f\"\\nExemplo de tag:\")\n",
        "print(tag_data[0])\n",
        "print(\"/n\")\n",
        "\n",
        "# Split treino/teste para tags\n",
        "random.shuffle(tag_data)\n",
        "split_idx = int(len(tag_data) * 0.9)\n",
        "tag_train_data = tag_data[:split_idx]\n",
        "tag_test_data = tag_data[split_idx:]\n",
        "\n",
        "print(f\"Tags treino: {len(tag_train_data)}\")\n",
        "print(f\"Tags teste: {len(tag_test_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-LaEGBNrzP2"
      },
      "source": [
        "## 4. Dataset Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JNQKLu1rzP2"
      },
      "outputs": [],
      "source": [
        "class MovieRecommendationDataset(Dataset):\n",
        "    \"\"\"Dataset para recomenda√ß√£o de filmes (tarefa principal)\"\"\"\n",
        "\n",
        "    def __init__(self, data, tokenizer, num_movies, max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.num_movies = num_movies\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        # Tokenizar input\n",
        "        encoding = self.tokenizer(\n",
        "            item['input_text'],\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Criar label multi-hot\n",
        "        labels = torch.zeros(self.num_movies)\n",
        "        for movie_idx in item['recommended_movies']:\n",
        "            if 0 <= movie_idx < self.num_movies:\n",
        "                labels[movie_idx] = 1.0\n",
        "\n",
        "        # Filmes mencionados (para RNN)\n",
        "        mentioned = item['mentioned_movies'][:20]  # Limitar\n",
        "        mentioned_tensor = torch.zeros(20, dtype=torch.long)\n",
        "        mentioned_mask = torch.zeros(20, dtype=torch.bool)\n",
        "\n",
        "        for i, m_idx in enumerate(mentioned):\n",
        "            if i < 20 and 0 <= m_idx < self.num_movies:\n",
        "                mentioned_tensor[i] = m_idx\n",
        "                mentioned_mask[i] = True\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'labels': labels,\n",
        "            'mentioned_movies': mentioned_tensor,\n",
        "            'mentioned_mask': mentioned_mask\n",
        "        }\n",
        "\n",
        "\n",
        "class TagDataset(Dataset):\n",
        "    \"\"\"Dataset para predi√ß√£o de filme a partir de tag (tarefa auxiliar)\"\"\"\n",
        "\n",
        "    def __init__(self, data, tokenizer, max_length=64):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            item['tag_text'],\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(item['movie_idx'], dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYnXsEbcrzP2"
      },
      "outputs": [],
      "source": [
        "# Inicializar tokenizer para SBERT\n",
        "sbert_tokenizer = AutoTokenizer.from_pretrained(config.sbert_model_name)\n",
        "print(f\"Cria o tokenizador do Modelo: {config.sbert_model_name}\")\n",
        "\n",
        "# Atualizar n√∫mero de filmes baseado no mapeamento real\n",
        "config.num_movies = movie_mapper.get_num_movies()\n",
        "print(f\"N√∫mero de filmes: {config.num_movies}\")\n",
        "\n",
        "# Criar datasets\n",
        "train_dataset = MovieRecommendationDataset(\n",
        "    train_data, sbert_tokenizer, config.num_movies, config.max_seq_length\n",
        ")\n",
        "test_dataset = MovieRecommendationDataset(\n",
        "    test_data, sbert_tokenizer, config.num_movies, config.max_seq_length\n",
        ")\n",
        "\n",
        "tag_train_dataset = TagDataset(tag_train_data, sbert_tokenizer)\n",
        "tag_test_dataset = TagDataset(tag_test_data, sbert_tokenizer)\n",
        "\n",
        "print(f\"\\nDataset de treino: {len(train_dataset)} exemplos\")\n",
        "print(f\"Dataset de teste: {len(test_dataset)} exemplos\")\n",
        "print(f\"Dataset de tags treino: {len(tag_train_dataset)} exemplos\")\n",
        "\n",
        "# Diagn√≥stico de configura√ß√£o\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DIAGN√ìSTICO DE TREINAMENTO\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Batches por √©poca (movies): {len(train_dataset) // config.movies_batch_size}\")\n",
        "print(f\"Total de steps (√©pocas): {(len(train_dataset) // config.movies_batch_size) * config.num_epochs}\")\n",
        "print(f\"Warmup steps (10%): {int((len(train_dataset) // config.movies_batch_size) * config.num_epochs * 0.1)}\")\n",
        "print(f\"\\nEspa√ßo de classifica√ß√£o: {config.num_movies} filmes poss√≠veis\")\n",
        "print(f\"M√©dia de labels positivos por exemplo: {sum(len(d['recommended_movies']) for d in train_data) / len(train_data):.2f}\")\n",
        "print(f\"Taxa de desbalanceamento: {config.num_movies / (sum(len(d['recommended_movies']) for d in train_data) / len(train_data)):.1f}:1\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwkJ0yNgrzP2"
      },
      "source": [
        "## 5. Arquitetura dos Modelos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri8m01PDTVpi"
      },
      "source": [
        "### 5.1 Modelo Baseline: SBERT + FFN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V0YwgyIrzP2"
      },
      "outputs": [],
      "source": [
        "class SBERTMovieRecommender(nn.Module):\n",
        "    \"\"\"\n",
        "    Modelo baseline: SBERT + FFN para classifica√ß√£o multi-label.\n",
        "\n",
        "    f(U) = FFN(SBERT_mean_pooling(U))\n",
        "\n",
        "    onde U √© o input concatenado com [SEP] tokens.\n",
        "    SBERT usa mean pooling dos tokens ao inv√©s de apenas [CLS].\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # Carregar modelo SBERT base\n",
        "        self.sbert = AutoModel.from_pretrained(config.sbert_model_name)\n",
        "\n",
        "        # FFN para proje√ß√£o\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(config.dropout_prob),\n",
        "            nn.Linear(config.sbert_hidden_size, config.ffn_hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.dropout_prob),\n",
        "            nn.Linear(config.ffn_hidden_size, config.num_movies)\n",
        "        )\n",
        "\n",
        "    def mean_pooling(self, token_embeddings, attention_mask):\n",
        "        \"\"\"Mean pooling - considera attention mask para m√©dia correta\"\"\"\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "        return sum_embeddings / sum_mask\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, **kwargs):\n",
        "        # Encoding SBERT\n",
        "        outputs = self.sbert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        # Mean pooling (caracter√≠stica do SBERT)\n",
        "        sentence_embeddings = self.mean_pooling(outputs.last_hidden_state, attention_mask)\n",
        "\n",
        "        # Proje√ß√£o para logits\n",
        "        logits = self.classifier(sentence_embeddings)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def get_sentence_embedding(self, input_ids, attention_mask):\n",
        "        \"\"\"Retorna sentence embedding para multi-task\"\"\"\n",
        "        outputs = self.sbert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        return self.mean_pooling(outputs.last_hidden_state, attention_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcbblvYnrzP2"
      },
      "source": [
        "### 5.2 Modelo com RNN para Features Colaborativas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OucrIy1irzP2"
      },
      "outputs": [],
      "source": [
        "class SBERTRNNMovieRecommender(nn.Module):\n",
        "    \"\"\"\n",
        "    Modelo com RNN para aprender features colaborativas.\n",
        "\n",
        "    f(U) = FFN(SBERT_mean_pooling(U), RNN(L(U)))\n",
        "\n",
        "    onde L(U) √© a lista de filmes mencionados em U.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.sbert = AutoModel.from_pretrained(config.sbert_model_name)\n",
        "\n",
        "        # Embedding de filmes para RNN\n",
        "        self.movie_embedding = nn.Embedding(\n",
        "            config.num_movies + 1,  # +1 para padding\n",
        "            config.rnn_embedding_size,\n",
        "            padding_idx=config.num_movies\n",
        "        )\n",
        "\n",
        "        # RNN para processar sequ√™ncia de filmes mencionados\n",
        "        self.rnn = nn.GRU(\n",
        "            input_size=config.rnn_embedding_size,\n",
        "            hidden_size=config.rnn_hidden_size,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Dimens√£o combinada: SBERT embedding + RNN output (bidirectional)\n",
        "        combined_size = config.sbert_hidden_size + (config.rnn_hidden_size * 2)\n",
        "\n",
        "        # FFN para proje√ß√£o\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(config.dropout_prob),\n",
        "            nn.Linear(combined_size, config.ffn_hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.dropout_prob),\n",
        "            nn.Linear(config.ffn_hidden_size, config.num_movies)\n",
        "        )\n",
        "\n",
        "        self.num_movies = config.num_movies\n",
        "\n",
        "    def mean_pooling(self, token_embeddings, attention_mask):\n",
        "        \"\"\"Mean pooling - considera attention mask para m√©dia correta\"\"\"\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "        return sum_embeddings / sum_mask\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, mentioned_movies, mentioned_mask, **kwargs):\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        # Encoding SBERT\n",
        "        sbert_outputs = self.sbert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        sentence_embeddings = self.mean_pooling(sbert_outputs.last_hidden_state, attention_mask)\n",
        "\n",
        "        # Processar filmes mencionados com RNN\n",
        "        # Substituir √≠ndices inv√°lidos pelo √≠ndice de padding\n",
        "        mentioned_movies = mentioned_movies.clone()\n",
        "        mentioned_movies[~mentioned_mask] = self.num_movies\n",
        "\n",
        "        movie_embeds = self.movie_embedding(mentioned_movies)\n",
        "\n",
        "        # RNN\n",
        "        rnn_output, hidden = self.rnn(movie_embeds)\n",
        "\n",
        "        # Usar √∫ltimo hidden state (concatenado de ambas dire√ß√µes)\n",
        "        rnn_features = hidden.transpose(0, 1).contiguous().view(batch_size, -1)\n",
        "\n",
        "        # Combinar features\n",
        "        combined = torch.cat([sentence_embeddings, rnn_features], dim=-1)\n",
        "\n",
        "        # Proje√ß√£o para logits\n",
        "        logits = self.classifier(combined)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def get_sentence_embedding(self, input_ids, attention_mask):\n",
        "        \"\"\"Retorna sentence embedding para multi-task\"\"\"\n",
        "        outputs = self.sbert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        return self.mean_pooling(outputs.last_hidden_state, attention_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKyimV0KrzP3"
      },
      "source": [
        "### 5.3 Wrapper para Multi-Task Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mn7QJfChrzP3"
      },
      "outputs": [],
      "source": [
        "class MultiTaskWrapper(nn.Module):\n",
        "    \"\"\"\n",
        "    Wrapper para multi-task learning com tarefa auxiliar de tags.\n",
        "\n",
        "    Loss = BCE(f(U), y) + CE(f(U), z)\n",
        "\n",
        "    onde z √© o filme correto para uma tag.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_model, config):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "\n",
        "        # Head separado para tarefa de tags (usa mesmo pathway do SBERT)\n",
        "        self.tag_classifier = nn.Sequential(\n",
        "            nn.Dropout(config.dropout_prob),\n",
        "            nn.Linear(config.sbert_hidden_size, config.ffn_hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.dropout_prob),\n",
        "            nn.Linear(config.ffn_hidden_size, config.num_movies)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, **kwargs):\n",
        "        return self.base_model(input_ids, attention_mask, **kwargs)\n",
        "\n",
        "    def forward_tags(self, input_ids, attention_mask):\n",
        "        \"\"\"Forward pass para tarefa de tags\"\"\"\n",
        "        # Obter sentence embedding do modelo base\n",
        "        if hasattr(self.base_model, 'get_sentence_embedding'):\n",
        "            embeddings = self.base_model.get_sentence_embedding(input_ids, attention_mask)\n",
        "        else:\n",
        "            embeddings = self.base_model.get_sentence_embedding(input_ids, attention_mask)\n",
        "\n",
        "        logits = self.tag_classifier(embeddings)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0egapTHcrzP3"
      },
      "source": [
        "## 6. M√©tricas de Avalia√ß√£o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIqgo5V4rzP3"
      },
      "outputs": [],
      "source": [
        "def compute_ndcg_at_k(predictions, labels, k=10):\n",
        "    \"\"\"\n",
        "    Calcula nDCG@k conforme usado no artigo.\n",
        "\n",
        "    Args:\n",
        "        predictions: tensor de logits (batch_size, num_movies)\n",
        "        labels: tensor multi-hot de labels (batch_size, num_movies)\n",
        "        k: n√∫mero de itens para considerar\n",
        "\n",
        "    Returns:\n",
        "        nDCG@k m√©dio\n",
        "    \"\"\"\n",
        "    predictions = predictions.detach().cpu().numpy()\n",
        "    labels = labels.detach().cpu().numpy()\n",
        "\n",
        "    ndcg_scores = []\n",
        "\n",
        "    for pred, label in zip(predictions, labels):\n",
        "        # Se n√£o h√° labels positivos, pular\n",
        "        if label.sum() == 0:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            score = ndcg_score([label], [pred], k=k)\n",
        "            ndcg_scores.append(score)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    return np.mean(ndcg_scores) if ndcg_scores else 0.0\n",
        "\n",
        "\n",
        "def compute_recall_at_k(predictions, labels, k=10):\n",
        "    \"\"\"Calcula Recall@k\"\"\"\n",
        "    predictions = predictions.detach().cpu().numpy()\n",
        "    labels = labels.detach().cpu().numpy()\n",
        "\n",
        "    recalls = []\n",
        "\n",
        "    for pred, label in zip(predictions, labels):\n",
        "        if label.sum() == 0:\n",
        "            continue\n",
        "\n",
        "        top_k_indices = np.argsort(pred)[-k:]\n",
        "        relevant = label[top_k_indices].sum()\n",
        "        total_relevant = label.sum()\n",
        "\n",
        "        recalls.append(relevant / total_relevant)\n",
        "\n",
        "    return np.mean(recalls) if recalls else 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuC_leOxrzP3"
      },
      "source": [
        "## 7. Loop de Treinamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Weighted Trainer (Solu√ß√£o para Class Imbalance)\n",
        "\n",
        "**Problemas identificados nos experimentos:**\n",
        "\n",
        "1. **BCE Loss sem pesos** ‚Üí modelo aprende a predizer 0 para tudo (nDCG@10 ‚âà 0.044)\n",
        "2. **Multi-task desbalanceado** ‚Üí CE loss (‚âà6.2) domina BCE loss (‚âà0.003)\n",
        "\n",
        "**Solu√ß√µes implementadas:**\n",
        "\n",
        "‚úÖ **Weighted BCE Loss**: `pos_weight=1200` para balancear 1:1200 classes  \n",
        "‚úÖ **Multi-task balanceado**: `loss = bce_loss + (0.001 * ce_loss)` para equalizar contribui√ß√µes  \n",
        "‚úÖ **Monitoramento separado**: Track BCE e CE losses individualmente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WeightedTrainer:\n",
        "    \"\"\"\n",
        "    Trainer com loss balanceado para resolver problemas de class imbalance\n",
        "    e multi-task desbalanceado identificados nos experimentos.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, config, train_loader, eval_loader,\n",
        "                 tag_train_loader=None, tag_eval_loader=None,\n",
        "                 use_multitask=False):\n",
        "        self.model = model.to(device)\n",
        "        self.config = config\n",
        "        self.train_loader = train_loader\n",
        "        self.eval_loader = eval_loader\n",
        "        self.tag_train_loader = tag_train_loader\n",
        "        self.tag_eval_loader = tag_eval_loader\n",
        "        self.use_multitask = use_multitask\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = AdamW(\n",
        "            model.parameters(),\n",
        "            lr=config.learning_rate\n",
        "        )\n",
        "\n",
        "        # Scheduler\n",
        "        total_steps = len(train_loader) * config.num_epochs\n",
        "        warmup_steps = int(total_steps * config.warmup_ratio)\n",
        "\n",
        "        self.scheduler = get_linear_schedule_with_warmup(\n",
        "            self.optimizer,\n",
        "            num_warmup_steps=warmup_steps,\n",
        "            num_training_steps=total_steps\n",
        "        )\n",
        "\n",
        "        # ‚úÖ SOLU√á√ÉO 1: Weighted BCE Loss para class imbalance\n",
        "        # Calcular pos_weight baseado no desbalanceamento real\n",
        "        avg_labels = sum(len(d['recommended_movies']) for d in train_data) / len(train_data)\n",
        "        pos_weight_value = config.num_movies / avg_labels  # ‚âà 1200\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"WEIGHTED TRAINER - CONFIGURA√á√ÉO\")\n",
        "        print('='*60)\n",
        "        print(f\"M√©dia de labels positivos: {avg_labels:.2f}\")\n",
        "        print(f\"Total de filmes: {config.num_movies}\")\n",
        "        print(f\"Desbalanceamento: {pos_weight_value:.1f}:1\")\n",
        "        print(f\"‚úÖ pos_weight aplicado: {pos_weight_value:.1f}\")\n",
        "        print('='*60)\n",
        "\n",
        "        pos_weight = torch.full([config.num_movies], pos_weight_value, device=device)\n",
        "        self.bce_loss = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        # ‚úÖ SOLU√á√ÉO 2: Balancear multi-task losses\n",
        "        # CE loss √© ~2000x maior que BCE, ent√£o usar alpha=0.001\n",
        "        self.multitask_alpha = 0.001  # Peso para CE loss\n",
        "\n",
        "        # Hist√≥rico expandido para monitorar losses separadamente\n",
        "        self.history = {\n",
        "            'train_loss': [],\n",
        "            'train_bce_loss': [],\n",
        "            'train_ce_loss': [],\n",
        "            'eval_loss': [],\n",
        "            'ndcg': [],\n",
        "            'recall': []\n",
        "        }\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        total_bce_loss = 0\n",
        "        total_ce_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        # Iterator para tags se multi-task\n",
        "        tag_iter = iter(self.tag_train_loader) if self.use_multitask and self.tag_train_loader else None\n",
        "\n",
        "        progress_bar = tqdm(self.train_loader, desc=\"Training\")\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            # Move batch para device\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass principal\n",
        "            logits = self.model(\n",
        "                input_ids=batch['input_ids'],\n",
        "                attention_mask=batch['attention_mask'],\n",
        "                mentioned_movies=batch.get('mentioned_movies'),\n",
        "                mentioned_mask=batch.get('mentioned_mask')\n",
        "            )\n",
        "\n",
        "            # Loss principal (BCE multi-label) com pesos\n",
        "            bce_loss = self.bce_loss(logits, batch['labels'])\n",
        "            loss = bce_loss\n",
        "\n",
        "            # Multi-task: adicionar loss de tags COM BALANCEAMENTO\n",
        "            ce_loss_value = 0\n",
        "            if self.use_multitask and tag_iter:\n",
        "                try:\n",
        "                    tag_batch = next(tag_iter)\n",
        "                except StopIteration:\n",
        "                    tag_iter = iter(self.tag_train_loader)\n",
        "                    tag_batch = next(tag_iter)\n",
        "\n",
        "                tag_batch = {k: v.to(device) for k, v in tag_batch.items()}\n",
        "\n",
        "                tag_logits = self.model.forward_tags(\n",
        "                    input_ids=tag_batch['input_ids'],\n",
        "                    attention_mask=tag_batch['attention_mask']\n",
        "                )\n",
        "\n",
        "                ce_loss = self.ce_loss(tag_logits, tag_batch['label'])\n",
        "                ce_loss_value = ce_loss.item()\n",
        "\n",
        "                # ‚úÖ Balancear: BCE loss (‚âà0.003) + alpha * CE loss (‚âà6.0)\n",
        "                # Com alpha=0.001: 0.003 + 0.001*6.0 = 0.009 (balanceado!)\n",
        "                loss = loss + (self.multitask_alpha * ce_loss)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "\n",
        "            self.optimizer.step()\n",
        "            self.scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_bce_loss += bce_loss.item()\n",
        "            total_ce_loss += ce_loss_value\n",
        "            num_batches += 1\n",
        "\n",
        "            # Mostrar losses separadamente\n",
        "            if self.use_multitask:\n",
        "                progress_bar.set_postfix({\n",
        "                    'loss': f'{loss.item():.4f}',\n",
        "                    'bce': f'{bce_loss.item():.4f}',\n",
        "                    'ce': f'{ce_loss_value:.4f}'\n",
        "                })\n",
        "            else:\n",
        "                progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        return {\n",
        "            'total': total_loss / num_batches,\n",
        "            'bce': total_bce_loss / num_batches,\n",
        "            'ce': total_ce_loss / num_batches if self.use_multitask else 0\n",
        "        }\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "\n",
        "        for batch in tqdm(self.eval_loader, desc=\"Evaluating\"):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            logits = self.model(\n",
        "                input_ids=batch['input_ids'],\n",
        "                attention_mask=batch['attention_mask'],\n",
        "                mentioned_movies=batch.get('mentioned_movies'),\n",
        "                mentioned_mask=batch.get('mentioned_mask')\n",
        "            )\n",
        "\n",
        "            loss = self.bce_loss(logits, batch['labels'])\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            all_predictions.append(logits)\n",
        "            all_labels.append(batch['labels'])\n",
        "\n",
        "        # Concatenar todas as predi√ß√µes\n",
        "        all_predictions = torch.cat(all_predictions, dim=0)\n",
        "        all_labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "        # Calcular m√©tricas\n",
        "        ndcg = compute_ndcg_at_k(all_predictions, all_labels, k=self.config.eval_k)\n",
        "        recall = compute_recall_at_k(all_predictions, all_labels, k=self.config.eval_k)\n",
        "\n",
        "        return {\n",
        "            'loss': total_loss / len(self.eval_loader),\n",
        "            'ndcg@10': ndcg,\n",
        "            'recall@10': recall\n",
        "        }\n",
        "\n",
        "    def train(self, num_epochs=None):\n",
        "        num_epochs = num_epochs or self.config.num_epochs\n",
        "        best_ndcg = 0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"\\n{'='*50}\")\n",
        "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "            print('='*50)\n",
        "\n",
        "            # Treinar\n",
        "            train_metrics = self.train_epoch()\n",
        "            self.history['train_loss'].append(train_metrics['total'])\n",
        "            self.history['train_bce_loss'].append(train_metrics['bce'])\n",
        "            self.history['train_ce_loss'].append(train_metrics['ce'])\n",
        "\n",
        "            # Avaliar\n",
        "            eval_metrics = self.evaluate()\n",
        "            self.history['eval_loss'].append(eval_metrics['loss'])\n",
        "            self.history['ndcg'].append(eval_metrics['ndcg@10'])\n",
        "            self.history['recall'].append(eval_metrics['recall@10'])\n",
        "\n",
        "            # Print com losses separadas\n",
        "            print(f\"\\nTrain Loss (total): {train_metrics['total']:.4f}\")\n",
        "            print(f\"  ‚îú‚îÄ BCE Loss: {train_metrics['bce']:.4f}\")\n",
        "            if self.use_multitask:\n",
        "                print(f\"  ‚îî‚îÄ CE Loss (x{self.multitask_alpha}): {train_metrics['ce']:.4f}\")\n",
        "            print(f\"Eval Loss: {eval_metrics['loss']:.4f}\")\n",
        "            print(f\"nDCG@10: {eval_metrics['ndcg@10']:.4f}\")\n",
        "            print(f\"Recall@10: {eval_metrics['recall@10']:.4f}\")\n",
        "\n",
        "            # Salvar melhor modelo\n",
        "            if eval_metrics['ndcg@10'] > best_ndcg:\n",
        "                best_ndcg = eval_metrics['ndcg@10']\n",
        "                torch.save(\n",
        "                    self.model.state_dict(),\n",
        "                    os.path.join(self.config.save_dir, 'best_weighted_model.pt')\n",
        "                )\n",
        "                print(f\"‚úÖ Novo melhor modelo salvo! nDCG@10: {best_ndcg:.4f}\")\n",
        "\n",
        "        return self.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKqrpZZPrzP3"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    \"\"\"Trainer para os modelos de recomenda√ß√£o\"\"\"\n",
        "\n",
        "    def __init__(self, model, config, train_loader, eval_loader,\n",
        "                 tag_train_loader=None, tag_eval_loader=None,\n",
        "                 use_multitask=False):\n",
        "        self.model = model.to(device)\n",
        "        self.config = config\n",
        "        self.train_loader = train_loader\n",
        "        self.eval_loader = eval_loader\n",
        "        self.tag_train_loader = tag_train_loader\n",
        "        self.tag_eval_loader = tag_eval_loader\n",
        "        self.use_multitask = use_multitask\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = AdamW(\n",
        "            model.parameters(),\n",
        "            lr=config.learning_rate\n",
        "        )\n",
        "\n",
        "        # Scheduler\n",
        "        total_steps = len(train_loader) * config.num_epochs\n",
        "        warmup_steps = int(total_steps * config.warmup_ratio)\n",
        "\n",
        "        self.scheduler = get_linear_schedule_with_warmup(\n",
        "            self.optimizer,\n",
        "            num_warmup_steps=warmup_steps,\n",
        "            num_training_steps=total_steps\n",
        "        )\n",
        "\n",
        "        # Loss functions\n",
        "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Hist√≥rico\n",
        "        self.history = {\n",
        "            'train_loss': [],\n",
        "            'eval_loss': [],\n",
        "            'ndcg': [],\n",
        "            'recall': []\n",
        "        }\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        # Iterator para tags se multi-task\n",
        "        tag_iter = iter(self.tag_train_loader) if self.use_multitask and self.tag_train_loader else None\n",
        "\n",
        "        progress_bar = tqdm(self.train_loader, desc=\"Training\")\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            # Move batch para device\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass principal\n",
        "            logits = self.model(\n",
        "                input_ids=batch['input_ids'],\n",
        "                attention_mask=batch['attention_mask'],\n",
        "                mentioned_movies=batch.get('mentioned_movies'),\n",
        "                mentioned_mask=batch.get('mentioned_mask')\n",
        "            )\n",
        "\n",
        "            # Loss principal (BCE multi-label)\n",
        "            loss = self.bce_loss(logits, batch['labels'])\n",
        "\n",
        "            # Multi-task: adicionar loss de tags\n",
        "            if self.use_multitask and tag_iter:\n",
        "                try:\n",
        "                    tag_batch = next(tag_iter)\n",
        "                except StopIteration:\n",
        "                    tag_iter = iter(self.tag_train_loader)\n",
        "                    tag_batch = next(tag_iter)\n",
        "\n",
        "                tag_batch = {k: v.to(device) for k, v in tag_batch.items()}\n",
        "\n",
        "                tag_logits = self.model.forward_tags(\n",
        "                    input_ids=tag_batch['input_ids'],\n",
        "                    attention_mask=tag_batch['attention_mask']\n",
        "                )\n",
        "\n",
        "                tag_loss = self.ce_loss(tag_logits, tag_batch['label'])\n",
        "                loss = loss + tag_loss  # Peso igual conforme artigo\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "\n",
        "            self.optimizer.step()\n",
        "            self.scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        return total_loss / num_batches\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "\n",
        "        for batch in tqdm(self.eval_loader, desc=\"Evaluating\"):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            logits = self.model(\n",
        "                input_ids=batch['input_ids'],\n",
        "                attention_mask=batch['attention_mask'],\n",
        "                mentioned_movies=batch.get('mentioned_movies'),\n",
        "                mentioned_mask=batch.get('mentioned_mask')\n",
        "            )\n",
        "\n",
        "            loss = self.bce_loss(logits, batch['labels'])\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            all_predictions.append(logits)\n",
        "            all_labels.append(batch['labels'])\n",
        "\n",
        "        # Concatenar todas as predi√ß√µes\n",
        "        all_predictions = torch.cat(all_predictions, dim=0)\n",
        "        all_labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "        # Calcular m√©tricas\n",
        "        ndcg = compute_ndcg_at_k(all_predictions, all_labels, k=self.config.eval_k)\n",
        "        recall = compute_recall_at_k(all_predictions, all_labels, k=self.config.eval_k)\n",
        "\n",
        "        return {\n",
        "            'loss': total_loss / len(self.eval_loader),\n",
        "            'ndcg@10': ndcg,\n",
        "            'recall@10': recall\n",
        "        }\n",
        "\n",
        "    def train(self, num_epochs=None):\n",
        "        num_epochs = num_epochs or self.config.num_epochs\n",
        "        best_ndcg = 0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"\\n{'='*50}\")\n",
        "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "            print('='*50)\n",
        "\n",
        "            # Treinar\n",
        "            train_loss = self.train_epoch()\n",
        "            self.history['train_loss'].append(train_loss)\n",
        "\n",
        "            # Avaliar\n",
        "            eval_metrics = self.evaluate()\n",
        "            self.history['eval_loss'].append(eval_metrics['loss'])\n",
        "            self.history['ndcg'].append(eval_metrics['ndcg@10'])\n",
        "            self.history['recall'].append(eval_metrics['recall@10'])\n",
        "\n",
        "            print(f\"\\nTrain Loss: {train_loss:.4f}\")\n",
        "            print(f\"Eval Loss: {eval_metrics['loss']:.4f}\")\n",
        "            print(f\"nDCG@10: {eval_metrics['ndcg@10']:.4f}\")\n",
        "            print(f\"Recall@10: {eval_metrics['recall@10']:.4f}\")\n",
        "\n",
        "            # Salvar melhor modelo\n",
        "            if eval_metrics['ndcg@10'] > best_ndcg:\n",
        "                best_ndcg = eval_metrics['ndcg@10']\n",
        "                torch.save(\n",
        "                    self.model.state_dict(),\n",
        "                    os.path.join(self.config.save_dir, 'best_model.pt')\n",
        "                )\n",
        "                print(f\"Novo melhor modelo salvo! nDCG@10: {best_ndcg:.4f}\")\n",
        "\n",
        "        return self.history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNds_WmjrzP3"
      },
      "source": [
        "## 8. Experimento 1: Baseline SBERT + FFN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGr2PuvzrzP3"
      },
      "outputs": [],
      "source": [
        "# Criar DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config.movies_batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,  # 0 para evitar problemas com multiprocessing no Colab\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "eval_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config.movies_batch_size * 2,\n",
        "    shuffle=False,\n",
        "    num_workers=0,  # 0 para evitar problemas com multiprocessing no Colab\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"Batches de treino: {len(train_loader)}\")\n",
        "print(f\"Batches de avalia√ß√£o: {len(eval_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "794rVjELrzP3"
      },
      "outputs": [],
      "source": [
        "# Treinar modelo baseline\n",
        "print(\"=\"*60)\n",
        "print(\"EXPERIMENTO 1: SBERT Baseline (sem RNN, sem multi-task)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"‚öôÔ∏è  Configura√ß√£o: batch_size={config.movies_batch_size}, lr={config.learning_rate}, dropout={config.dropout_prob}\")\n",
        "print(f\"üìä Observe: nDCG@10 deve come√ßar baixo (~0.01-0.02) e crescer gradualmente\")\n",
        "print(f\"üéØ Meta em {config.num_epochs} √©pocas: nDCG@10 > 0.05\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "baseline_model = SBERTMovieRecommender(config)\n",
        "\n",
        "baseline_trainer = Trainer(\n",
        "    model=baseline_model,\n",
        "    config=config,\n",
        "    train_loader=train_loader,\n",
        "    eval_loader=eval_loader,\n",
        "    use_multitask=False\n",
        ")\n",
        "\n",
        "# Treinar com hiperpar√¢metros ajustados\n",
        "baseline_history = baseline_trainer.train(config.num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukQ0vpS_rzP3"
      },
      "source": [
        "## 9. Experimento 2: SBERT + RNN para Features Colaborativas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5hgqNT3erzP3"
      },
      "outputs": [],
      "source": [
        "# Treinar modelo com RNN\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENTO 2: SBERT + RNN (features colaborativas)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "rnn_model = SBERTRNNMovieRecommender(config)\n",
        "\n",
        "rnn_trainer = Trainer(\n",
        "    model=rnn_model,\n",
        "    config=config,\n",
        "    train_loader=train_loader,\n",
        "    eval_loader=eval_loader,\n",
        "    use_multitask=False\n",
        ")\n",
        "\n",
        "rnn_history = rnn_trainer.train(config.num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld8boIMerzP3"
      },
      "source": [
        "## 10. Experimento 3: Multi-Task Learning com Tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUU7DpZirzP3"
      },
      "outputs": [],
      "source": [
        "# Criar DataLoaders para tags\n",
        "tag_train_loader = DataLoader(\n",
        "    tag_train_dataset,\n",
        "    batch_size=config.tags_batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,  # 0 para evitar problemas com multiprocessing no Colab\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "tag_eval_loader = DataLoader(\n",
        "    tag_test_dataset,\n",
        "    batch_size=config.tags_batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=0,  # 0 para evitar problemas com multiprocessing no Colab\n",
        "    pin_memory=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plrkpj03rzP3"
      },
      "outputs": [],
      "source": [
        "# Treinar modelo com multi-task (SBERT baseline + tags)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENTO 3: SBERT + Multi-Task (user tags)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "base_model_mt = SBERTMovieRecommender(config)\n",
        "multitask_model = MultiTaskWrapper(base_model_mt, config)\n",
        "\n",
        "multitask_trainer = Trainer(\n",
        "    model=multitask_model,\n",
        "    config=config,\n",
        "    train_loader=train_loader,\n",
        "    eval_loader=eval_loader,\n",
        "    tag_train_loader=tag_train_loader,\n",
        "    tag_eval_loader=tag_eval_loader,\n",
        "    use_multitask=True\n",
        ")\n",
        "\n",
        "multitask_history = multitask_trainer.train(config.num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaClvCTtrzP4"
      },
      "source": [
        "## 11. Experimento 4: SBERT + RNN + Multi-Task (Modelo Completo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "f2AYmZiTrzP4"
      },
      "outputs": [],
      "source": [
        "# Modelo completo: RNN + Multi-task\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENTO 4: SBERT + RNN + Multi-Task (modelo completo)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "rnn_base_model = SBERTRNNMovieRecommender(config)\n",
        "full_model = MultiTaskWrapper(rnn_base_model, config)\n",
        "\n",
        "full_trainer = Trainer(\n",
        "    model=full_model,\n",
        "    config=config,\n",
        "    train_loader=train_loader,\n",
        "    eval_loader=eval_loader,\n",
        "    tag_train_loader=tag_train_loader,\n",
        "    tag_eval_loader=tag_eval_loader,\n",
        "    use_multitask=True\n",
        ")\n",
        "\n",
        "full_history = full_trainer.train(config.num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXJvrCE4rzP4"
      },
      "source": [
        "## 12. Compara√ß√£o dos Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ty2TGCWrzP4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_results(histories, names):\n",
        "    \"\"\"Plota compara√ß√£o dos resultados\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "    colors = ['#2ecc71', '#3498db', '#e74c3c', '#9b59b6']\n",
        "\n",
        "    # Train Loss\n",
        "    ax = axes[0, 0]\n",
        "    for hist, name, color in zip(histories, names, colors):\n",
        "        ax.plot(hist['train_loss'], label=name, color=color, linewidth=2)\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.set_title('Training Loss')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Eval Loss\n",
        "    ax = axes[0, 1]\n",
        "    for hist, name, color in zip(histories, names, colors):\n",
        "        ax.plot(hist['eval_loss'], label=name, color=color, linewidth=2)\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.set_title('Evaluation Loss')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # nDCG@10\n",
        "    ax = axes[1, 0]\n",
        "    for hist, name, color in zip(histories, names, colors):\n",
        "        ax.plot(hist['ndcg'], label=name, color=color, linewidth=2)\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('nDCG@10')\n",
        "    ax.set_title('nDCG@10 (m√©trica principal do artigo)')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Recall@10\n",
        "    ax = axes[1, 1]\n",
        "    for hist, name, color in zip(histories, names, colors):\n",
        "        ax.plot(hist['recall'], label=name, color=color, linewidth=2)\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Recall@10')\n",
        "    ax.set_title('Recall@10')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_results.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Plotar resultados\n",
        "plot_results(\n",
        "    [baseline_history, rnn_history, multitask_history, full_history],\n",
        "    ['SBERT Baseline', 'SBERT + RNN', 'SBERT + Multi-Task', 'SBERT + RNN + Multi-Task']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lj9NYs_jrzP4"
      },
      "outputs": [],
      "source": [
        "\n",
        "results = {\n",
        "    'Modelo': [\n",
        "        'SBERT Baseline',\n",
        "        'SBERT + RNN',\n",
        "        'SBERT + Multi-Task',\n",
        "        'SBERT + RNN + Multi-Task',\n",
        "        '--- Artigo Original (BERT) ---',\n",
        "        'Artigo: Sem RNN, Sem Tags',\n",
        "        'Artigo: Com RNN, Sem Tags',\n",
        "        'Artigo: Sem RNN, Com Tags',\n",
        "        'Artigo: Com RNN, Com Tags'\n",
        "    ],\n",
        "    'nDCG@10': [\n",
        "        max(baseline_history['ndcg']),\n",
        "        max(rnn_history['ndcg']),\n",
        "        max(multitask_history['ndcg']),\n",
        "        max(full_history['ndcg']),\n",
        "        '-',\n",
        "        0.130,\n",
        "        0.165,\n",
        "        0.138,\n",
        "        0.169\n",
        "    ]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Nota: O artigo original usa BERT e reporta nDCG@10 de 0.819 para\")\n",
        "print(\"modelos conversacionais completos (tarefa diferente). Esta implementa√ß√£o\")\n",
        "print(\"usa SBERT e foca na tarefa de recomenda√ß√£o one-shot a partir de queries.\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlbxdCJtrzP4"
      },
      "source": [
        "## 13. Infer√™ncia e Demonstra√ß√£o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcnAWh-VrzP4"
      },
      "outputs": [],
      "source": [
        "class MovieRecommenderInference:\n",
        "    \"\"\"Classe para infer√™ncia com o modelo treinado\"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer, movie_mapper, device, top_k=10):\n",
        "        self.model = model.to(device)\n",
        "        self.model.eval()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.movie_mapper = movie_mapper\n",
        "        self.device = device\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def recommend(self, query, mentioned_movie_names=None):\n",
        "        \"\"\"\n",
        "        Gera recomenda√ß√µes a partir de uma query.\n",
        "\n",
        "        Args:\n",
        "            query: texto da query do usu√°rio\n",
        "            mentioned_movie_names: lista de nomes de filmes mencionados (opcional)\n",
        "\n",
        "        Returns:\n",
        "            Lista de filmes recomendados com scores\n",
        "        \"\"\"\n",
        "        # Tokenizar\n",
        "        encoding = self.tokenizer(\n",
        "            query,\n",
        "            max_length=512,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].to(self.device)\n",
        "        attention_mask = encoding['attention_mask'].to(self.device)\n",
        "\n",
        "        # Preparar filmes mencionados (placeholder se n√£o dispon√≠vel)\n",
        "        mentioned_movies = torch.zeros(1, 20, dtype=torch.long, device=self.device)\n",
        "        mentioned_mask = torch.zeros(1, 20, dtype=torch.bool, device=self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                mentioned_movies=mentioned_movies,\n",
        "                mentioned_mask=mentioned_mask\n",
        "            )\n",
        "\n",
        "        # Obter top-k\n",
        "        probs = torch.sigmoid(logits).squeeze(0)\n",
        "        top_scores, top_indices = torch.topk(probs, self.top_k)\n",
        "\n",
        "        recommendations = []\n",
        "        for score, idx in zip(top_scores.cpu().numpy(), top_indices.cpu().numpy()):\n",
        "            movie_id = self.movie_mapper.idx_to_movie_id(idx)\n",
        "            movie_name = self.movie_mapper.movie_names.get(movie_id, f\"Movie {movie_id}\")\n",
        "            recommendations.append({\n",
        "                'movie_id': movie_id,\n",
        "                'name': movie_name,\n",
        "                'score': float(score)\n",
        "            })\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "# Criar inference engine com o melhor modelo\n",
        "inference = MovieRecommenderInference(\n",
        "    model=full_model,\n",
        "    tokenizer=sbert_tokenizer,\n",
        "    movie_mapper=movie_mapper,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKPUKA1trzP4"
      },
      "outputs": [],
      "source": [
        "# Demonstra√ß√£o de infer√™ncia\n",
        "print(\"=\"*60)\n",
        "print(\"DEMONSTRA√á√ÉO DE RECOMENDA√á√ïES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_queries = [\n",
        "    \"I like animations and comedies. I enjoyed Toy Story and Finding Nemo.\",\n",
        "    \"I'm looking for something dramatic and artistic. I love Christopher Nolan films.\",\n",
        "    \"Can you recommend some action movies? I like Marvel superhero films.\",\n",
        "    \"I want to watch something scary for Halloween. Horror movies please!\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"\\n{'‚îÄ'*60}\")\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"{'‚îÄ'*60}\")\n",
        "\n",
        "    recommendations = inference.recommend(query)\n",
        "\n",
        "    print(\"\\nTop 5 Recomenda√ß√µes:\")\n",
        "    for i, rec in enumerate(recommendations[:5], 1):\n",
        "        print(f\"  {i}. {rec['name']} (score: {rec['score']:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTK_FbpLrzP5"
      },
      "source": [
        "## 14. An√°lise de Erros e Limita√ß√µes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzccv0NhrzP5"
      },
      "outputs": [],
      "source": [
        "# An√°lise conforme discutido no artigo\n",
        "print(\"=\"*70)\n",
        "print(\"AN√ÅLISE DE LIMITA√á√ïES (conforme artigo)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "analysis = \"\"\"\n",
        "1. TAMANHO DO DATASET\n",
        "   - Treino: {} exemplos\n",
        "   - Teste: {} exemplos\n",
        "   - O artigo menciona ~8008 treino e ~2002 avalia√ß√£o\n",
        "   - Dataset pequeno leva a overfitting\n",
        "\n",
        "2. QUALIDADE DOS DADOS\n",
        "   - Senten√ßas concatenadas de di√°logos podem n√£o ser significativas\n",
        "   - Exemplo: \"Anything artistic [SEP] What's it about?\" n√£o faz sentido isolado\n",
        "\n",
        "3. COBERTURA DE FILMES\n",
        "   - Total de filmes no mapeamento: {}\n",
        "   - Nem todos t√™m tags de usu√°rios para multi-task\n",
        "\n",
        "4. COMPARA√á√ÉO COM ARTIGO\n",
        "   - Artigo reporta nDCG@10 entre 0.130 e 0.169\n",
        "   - Modelos conversacionais completos atingem 0.819\n",
        "   - Nossa tarefa √© mais dif√≠cil (one-shot vs conversacional)\n",
        "\n",
        "5. MELHORIAS OBSERVADAS\n",
        "   - RNN para features colaborativas: +0.035 nDCG (artigo)\n",
        "   - Multi-task com tags: +0.004-0.008 nDCG (artigo)\n",
        "\"\"\".format(\n",
        "    len(train_data),\n",
        "    len(test_data),\n",
        "    movie_mapper.get_num_movies()\n",
        ")\n",
        "\n",
        "print(analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AX283pfrzP5"
      },
      "source": [
        "## 15. Salvar Modelo Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GFXCD--rzP5"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Salvar modelo completo e configura√ß√µes\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "class NumpyEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, (np.floating, np.integer)):\n",
        "            return obj.item()\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        elif hasattr(obj, 'item'):  # Para tensores PyTorch\n",
        "            return obj.item()\n",
        "        return super().default(obj)\n",
        "\n",
        "save_path = os.path.join(config.save_dir, 'final_model')\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Salvar pesos do modelo\n",
        "torch.save(full_model.state_dict(), os.path.join(save_path, 'model_weights.pt'))\n",
        "\n",
        "# Salvar configura√ß√µes\n",
        "config_dict = {k: v for k, v in vars(config).items() if not k.startswith('_')}\n",
        "with open(os.path.join(save_path, 'config.json'), 'w') as f:\n",
        "    json.dump(config_dict, f, indent=2, cls=NumpyEncoder)\n",
        "\n",
        "# Salvar mapeamento de filmes\n",
        "with open(os.path.join(save_path, 'movie_mapping.json'), 'w') as f:\n",
        "    json.dump({\n",
        "        'movie_to_idx': movie_mapper.movie_to_idx,\n",
        "        'movie_names': movie_mapper.movie_names\n",
        "    }, f, indent=2, cls=NumpyEncoder)\n",
        "\n",
        "# Salvar hist√≥rico de treinamento\n",
        "with open(os.path.join(save_path, 'training_history.json'), 'w') as f:\n",
        "    json.dump({\n",
        "        'baseline': baseline_history,\n",
        "        'rnn': rnn_history,\n",
        "        'multitask': multitask_history,\n",
        "        'full': full_history\n",
        "    }, f, indent=2, cls=NumpyEncoder)\n",
        "\n",
        "print(f\"Modelo salvo em: {save_path}\")\n",
        "print(\"Arquivos salvos:\")\n",
        "for f in os.listdir(save_path):\n",
        "    print(f\"  - {f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSg5HuLzJVL-"
      },
      "outputs": [],
      "source": [
        "# imprime relatorio com os valores encontrados\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RELAT√ìRIO FINAL\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOG0Lo6JrzP5"
      },
      "source": [
        "## 16. Conclus√£o\n",
        "\n",
        "Esta implementa√ß√£o adapta o artigo \"BERT one-shot movie recommender system\" de Trung Nguyen (Stanford CS224N) para usar SBERT (Sentence-BERT).\n",
        "\n",
        "### Resultados Principais:\n",
        "\n",
        "| Configura√ß√£o | nDCG@10 (Artigo BERT) | nDCG@10 (SBERT) |\n",
        "|-------------|------------------|----------------------|\n",
        "| Baseline | 0.130 | Veja resultados acima |\n",
        "| + RNN | 0.165 | Veja resultados acima |\n",
        "| + Multi-Task | 0.138 | Veja resultados acima |\n",
        "| + RNN + Multi-Task | 0.169 | Veja resultados acima |\n",
        "\n",
        "### Insights:\n",
        "1. O RNN para features colaborativas melhora significativamente os resultados\n",
        "2. Multi-task learning com tags oferece ganho marginal\n",
        "3. A combina√ß√£o de ambas t√©cnicas produz o melhor resultado\n",
        "4. SBERT usa mean pooling em vez de apenas [CLS], potencialmente capturando melhor o contexto\n",
        "5. O dataset pequeno e a natureza concatenada dos dados limitam o desempenho\n",
        "\n",
        "### Refer√™ncias:\n",
        "- Nguyen, T. (2024). BERT one-shot movie recommender system. Stanford CS224N.\n",
        "- Reimers & Gurevych (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. EMNLP.\n",
        "- Li et al. (2018). Towards deep conversational recommendations. NeurIPS.\n",
        "- Penha & Hauff (2020). What does BERT know about books, movies and music? RecSys."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 17. Experimentos com Weighted Trainer (Solu√ß√£o)\n",
        "\n",
        "**Objetivo:** Validar se as corre√ß√µes resolvem os problemas identificados:\n",
        "- ‚úÖ Weighted BCE Loss: `pos_weight=1200` para balancear classes\n",
        "- ‚úÖ Multi-task balanceado: `alpha=0.001` para equalizar BCE e CE losses\n",
        "\n",
        "**Expectativa:** nDCG@10 deve subir de **0.044** para **> 0.10** em 20 √©pocas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 17.1 Teste R√°pido: Baseline com Weighted BCE (5 √©pocas)\n",
        "\n",
        "**Objetivo:** Validar rapidamente se Weighted BCE resolve o problema principal.  \n",
        "**Esperado:** nDCG@10 deve ser **> 0.06** j√° em 5 √©pocas (vs 0.044 sem pesos)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Teste r√°pido: Baseline com Weighted BCE (5 √©pocas)\n",
        "print(\"=\"*60)\n",
        "print(\"TESTE: SBERT Baseline com Weighted BCE Loss\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Criar novo modelo (resetar pesos)\n",
        "weighted_baseline = SBERTMovieRecommender(config)\n",
        "\n",
        "# Usar WeightedTrainer\n",
        "weighted_baseline_trainer = WeightedTrainer(\n",
        "    model=weighted_baseline,\n",
        "    config=config,\n",
        "    train_loader=train_loader,\n",
        "    eval_loader=eval_loader,\n",
        "    use_multitask=False\n",
        ")\n",
        "\n",
        "# Treinar apenas 5 √©pocas para teste r√°pido\n",
        "weighted_baseline_history = weighted_baseline_trainer.train(num_epochs=5)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESULTADO DO TESTE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Melhor nDCG@10: {max(weighted_baseline_history['ndcg']):.4f}\")\n",
        "print(f\"Baseline original (20 √©pocas): 0.0440\")\n",
        "print(f\"Diferen√ßa: {(max(weighted_baseline_history['ndcg']) - 0.0440):.4f}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 17.2 Experimento Completo: RNN + Multi-Task com Weighted Trainer\n",
        "\n",
        "**Se o teste r√°pido funcionar**, rodar modelo completo com 20 √©pocas:\n",
        "- Weighted BCE Loss (pos_weight=1200)\n",
        "- Multi-task balanceado (alpha=0.001)\n",
        "- RNN para features colaborativas\n",
        "\n",
        "**Meta:** nDCG@10 **> 0.10** (vs 0.0462 do Exp 4 original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modelo completo: RNN + Multi-task com Weighted Trainer\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENTO FINAL: SBERT + RNN + Multi-Task (WEIGHTED)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Criar modelo completo\n",
        "weighted_rnn_base = SBERTRNNMovieRecommender(config)\n",
        "weighted_full_model = MultiTaskWrapper(weighted_rnn_base, config)\n",
        "\n",
        "# Usar WeightedTrainer com multi-task\n",
        "weighted_full_trainer = WeightedTrainer(\n",
        "    model=weighted_full_model,\n",
        "    config=config,\n",
        "    train_loader=train_loader,\n",
        "    eval_loader=eval_loader,\n",
        "    tag_train_loader=tag_train_loader,\n",
        "    tag_eval_loader=tag_eval_loader,\n",
        "    use_multitask=True\n",
        ")\n",
        "\n",
        "# Treinar 20 √©pocas completas\n",
        "weighted_full_history = weighted_full_trainer.train(config.num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 17.3 Compara√ß√£o: Original vs Weighted Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tabela comparativa\n",
        "comparison_results = {\n",
        "    'Configura√ß√£o': [\n",
        "        'Exp 1: Baseline (Original)',\n",
        "        'Exp 2: +RNN (Original)',\n",
        "        'Exp 3: +Multi-Task (Original)',\n",
        "        'Exp 4: +RNN+Multi-Task (Original)',\n",
        "        '‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ',\n",
        "        'Teste: Baseline (Weighted) - 5 √©pocas',\n",
        "        'Final: +RNN+Multi-Task (Weighted) - 20 √©pocas',\n",
        "        '‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ',\n",
        "        'Meta (Artigo): Baseline',\n",
        "        'Meta (Artigo): +RNN+Multi-Task'\n",
        "    ],\n",
        "    'Train Loss': [\n",
        "        '0.0035',\n",
        "        '0.0035',\n",
        "        '6.2071 ‚ùå',\n",
        "        '6.2516 ‚ùå',\n",
        "        '‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ',\n",
        "        f\"{weighted_baseline_history['train_loss'][-1]:.4f}\",\n",
        "        f\"{weighted_full_history['train_loss'][-1]:.4f}\",\n",
        "        '‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ',\n",
        "        '~0.005',\n",
        "        '~0.008'\n",
        "    ],\n",
        "    'nDCG@10': [\n",
        "        '0.0440',\n",
        "        '0.0459',\n",
        "        '0.0443',\n",
        "        '0.0462',\n",
        "        '‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ',\n",
        "        f\"{max(weighted_baseline_history['ndcg']):.4f}\",\n",
        "        f\"{max(weighted_full_history['ndcg']):.4f}\",\n",
        "        '‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ',\n",
        "        '0.130',\n",
        "        '0.169'\n",
        "    ],\n",
        "    'Status': [\n",
        "        '‚ùå Stagnado',\n",
        "        '‚ùå RNN in√∫til',\n",
        "        '‚ùå Multi-task quebrado',\n",
        "        '‚ùå Combina√ß√£o pior',\n",
        "        '‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ',\n",
        "        '‚úÖ Corrigido' if max(weighted_baseline_history['ndcg']) > 0.06 else '‚ö†Ô∏è Verificar',\n",
        "        '‚úÖ Corrigido' if max(weighted_full_history['ndcg']) > 0.10 else '‚ö†Ô∏è Verificar',\n",
        "        '‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ',\n",
        "        'Alvo',\n",
        "        'Alvo'\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_results)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARA√á√ÉO: TRAINER ORIGINAL vs WEIGHTED TRAINER\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"AN√ÅLISE DE GANHO\")\n",
        "print(\"=\"*80)\n",
        "original_best = 0.0462  # Exp 4 original\n",
        "weighted_best = max(weighted_full_history['ndcg'])\n",
        "improvement = ((weighted_best - original_best) / original_best) * 100\n",
        "\n",
        "print(f\"Melhor resultado original: {original_best:.4f}\")\n",
        "print(f\"Melhor resultado weighted: {weighted_best:.4f}\")\n",
        "print(f\"Ganho absoluto: +{(weighted_best - original_best):.4f}\")\n",
        "print(f\"Ganho percentual: +{improvement:.1f}%\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 18. Pr√≥ximos Passos e Recomenda√ß√µes\n",
        "\n",
        "### üéØ Como Usar Este Notebook\n",
        "\n",
        "#### **Op√ß√£o 1: Teste R√°pido (Recomendado para iniciar)**\n",
        "Execute a c√©lula da se√ß√£o **17.1** para validar o Weighted Trainer em 5 √©pocas:\n",
        "```python\n",
        "# Deve levar ~5 minutos no Colab com GPU\n",
        "weighted_baseline_trainer.train(num_epochs=5)\n",
        "```\n",
        "\n",
        "**Crit√©rio de sucesso**: nDCG@10 **> 0.06** j√° em 5 √©pocas (vs 0.044 original).\n",
        "\n",
        "---\n",
        "\n",
        "#### **Op√ß√£o 2: Experimento Completo**\n",
        "Se o teste r√°pido funcionar, execute a c√©lula da se√ß√£o **17.2** para treinar o modelo completo:\n",
        "```python\n",
        "# Leva ~20 minutos no Colab com GPU\n",
        "weighted_full_trainer.train(num_epochs=20)\n",
        "```\n",
        "\n",
        "**Crit√©rio de sucesso**: nDCG@10 **> 0.10** em 20 √©pocas (alvo: 0.13-0.17 do artigo).\n",
        "\n",
        "---\n",
        "\n",
        "### üîß Ajustes Adicionais (Se Necess√°rio)\n",
        "\n",
        "#### **Se nDCG@10 ainda estiver baixo (< 0.08 em 20 √©pocas):**\n",
        "\n",
        "1. **Aumentar pos_weight** (linha 831 do WeightedTrainer):\n",
        "   ```python\n",
        "   pos_weight_value = config.num_movies / avg_labels * 1.5  # Aumentar 50%\n",
        "   ```\n",
        "\n",
        "2. **Ajustar alpha do multi-task** (linha 851):\n",
        "   ```python\n",
        "   self.multitask_alpha = 0.0005  # Reduzir peso do CE loss\n",
        "   ```\n",
        "\n",
        "3. **Implementar Focal Loss** (alternativa ao Weighted BCE):\n",
        "   ```python\n",
        "   # Foca em exemplos dif√≠ceis, n√£o apenas balanceamento\n",
        "   class FocalLoss(nn.Module):\n",
        "       def __init__(self, alpha=1, gamma=2):\n",
        "           super().__init__()\n",
        "           self.alpha = alpha\n",
        "           self.gamma = gamma\n",
        "       \n",
        "       def forward(self, inputs, targets):\n",
        "           bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
        "           pt = torch.exp(-bce_loss)\n",
        "           focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss\n",
        "           return focal_loss.mean()\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "### üìä Interpretando os Resultados\n",
        "\n",
        "**Sinais de que est√° funcionando:**\n",
        "- ‚úÖ Train Loss entre 0.01-0.03 (n√£o mais 6.2)\n",
        "- ‚úÖ BCE Loss e CE Loss balanceados (~mesma ordem de magnitude)\n",
        "- ‚úÖ nDCG@10 crescendo consistentemente\n",
        "- ‚úÖ Gap pequeno entre Train Loss e Eval Loss (<0.01)\n",
        "\n",
        "**Sinais de problema persistente:**\n",
        "- ‚ùå Train Loss ainda muito alta (>1.0) ap√≥s 10 √©pocas\n",
        "- ‚ùå nDCG@10 estagnado em <0.05 ap√≥s 15 √©pocas\n",
        "- ‚ùå BCE Loss muito maior que CE Loss (desbalanceamento inverso)\n",
        "\n",
        "---\n",
        "\n",
        "### üìà Benchmarks Esperados\n",
        "\n",
        "| √âpocas | nDCG@10 (Weighted) | Status |\n",
        "|--------|-------------------|--------|\n",
        "| 5 | 0.06-0.08 | ‚úÖ Valida√ß√£o inicial |\n",
        "| 10 | 0.08-0.10 | üîÑ Progresso |\n",
        "| 20 | 0.10-0.13 | üéØ Meta m√≠nima |\n",
        "| 50+ | 0.13-0.17 | üèÜ Meta do artigo |\n",
        "\n",
        "---\n",
        "\n",
        "### üêõ Troubleshooting\n",
        "\n",
        "**\"CUDA out of memory\":**\n",
        "```python\n",
        "config.movies_batch_size = 16  # Reduzir de 32\n",
        "```\n",
        "\n",
        "**\"Loss NaN\":**\n",
        "```python\n",
        "# Reduzir learning rate\n",
        "config.learning_rate = 1e-5  # De 2e-5\n",
        "```\n",
        "\n",
        "**\"Overfitting (Train Loss << Eval Loss)\":**\n",
        "```python\n",
        "# Aumentar dropout\n",
        "config.dropout_prob = 0.2  # De 0.1\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzP5gJftrzP5"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"IMPLEMENTA√á√ÉO SBERT COMPLETA!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nPara continuar o treinamento com mais √©pocas, ajuste:\")\n",
        "print(\"  config.num_epochs = 200  # Conforme artigo original\")\n",
        "print(\"\\nPara usar o modelo treinado:\")\n",
        "print(\"  inference = MovieRecommenderInference(full_model, sbert_tokenizer, movie_mapper, device)\")\n",
        "print(\"  recs = inference.recommend('I like action movies')\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "x9ml9dNSrzPz",
        "HDrse0LLrzP0",
        "qI3m0bgxTjSy",
        "0mvwZO-SEGIz",
        "Brpc9myKrzP1"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

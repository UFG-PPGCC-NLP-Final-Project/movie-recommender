{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UFG-PPGCC-NLP-Final-Project/movie-recommender/blob/main/colab/sbert_movie_recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emp9JbsOrzPx"
      },
      "source": [
        "# SBERT Movie Recommender System\n",
        "\n",
        "**Adaptação do artigo**: *BERT one-shot movie recommender system* usando Sentence-BERT\n",
        "\n",
        "Este notebook implementa um sistema de recomendação de filmes end-to-end usando SBERT (Sentence-BERT), projetado para produzir recomendações estruturadas a partir de queries não estruturadas.\n",
        "\n",
        "## Arquitetura\n",
        "- **Baseline**: SBERT + FFN para classificação multi-label\n",
        "- **Extensão 1**: SBERT + RNN para features colaborativas\n",
        "- **Extensão 2**: Multi-task learning com dados de tags de usuários\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9ml9dNSrzPz"
      },
      "source": [
        "## 1. Configuração do Ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MRd55n-xrzPz"
      },
      "outputs": [],
      "source": [
        "# Verificar GPU disponível\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ORJobH3LrzP0"
      },
      "outputs": [],
      "source": [
        "# Instalar dependências\n",
        "!pip install -q sentence-transformers transformers datasets torch accelerate scikit-learn pandas numpy tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JF2beBfgrzP0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import ndcg_score\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Configurar seeds para reprodutibilidade\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Configurar device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Usando device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memória total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDrse0LLrzP0"
      },
      "source": [
        "## 2. Configurações e Hiperparâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8K21S8x6rzP0"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    \"\"\"Configurações do modelo e treinamento\"\"\"\n",
        "\n",
        "    # Modelo SBERT\n",
        "    sbert_model_name = 'sentence-transformers/all-mpnet-base-v2'  # Melhor modelo SBERT disponível\n",
        "    sbert_hidden_size = 768  # Dimensão do all-mpnet-base-v2 (2x mais que MiniLM)\n",
        "\n",
        "    # RNN para features colaborativas\n",
        "    rnn_embedding_size = 256\n",
        "    rnn_hidden_size = 128\n",
        "\n",
        "    # FFN\n",
        "    ffn_hidden_size = 256\n",
        "    dropout_prob = 0.3\n",
        "\n",
        "    # Treinamento\n",
        "    movies_batch_size = 16  # Aumentado de 8 para 16 (melhor estabilidade de gradiente)\n",
        "    tags_batch_size = 64    # # Conforme artigo\n",
        "    learning_rate = 1e-5    # Conforme artigo\n",
        "    num_epochs = 5          # Reduzido para demonstração (artigo usa 200)\\n\",\n",
        "    warmup_ratio = 0.1\n",
        "    max_seq_length = 512\n",
        "\n",
        "    # Dataset\n",
        "    num_movies = None  # Será definido automaticamente pelo movie_mapper.get_num_movies()\n",
        "\n",
        "    # Avaliação\n",
        "    eval_k = 10            # nDCG@10\n",
        "\n",
        "    # Checkpoints\n",
        "    save_dir = './checkpoints'\n",
        "\n",
        "config = Config()\n",
        "os.makedirs(config.save_dir, exist_ok=True)\n",
        "\n",
        "# Imprimir configurações importantes\n",
        "print(\"=\"*60)\n",
        "print(\"HIPERPARÂMETROS CONFIGURADOS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Learning Rate: {config.learning_rate}\")\n",
        "print(f\"Dropout: {config.dropout_prob}\")\n",
        "print(f\"Movies Batch Size: {config.movies_batch_size}\")\n",
        "print(f\"Tags Batch Size: {config.tags_batch_size}\")\n",
        "print(f\"Num Epochs: {config.num_epochs}\")\n",
        "print(f\"Max Seq Length: {config.max_seq_length}\")\n",
        "print(f\"Num Movies: {config.num_movies} (será definido automaticamente)\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Explicação dos Hiperparâmetros Ajustados\n",
        "\n",
        "**Mudanças em relação à configuração original para melhorar resultados:**\n",
        "\n",
        "#### 1. **Learning Rate: 1e-5 → 2e-5** (2x maior)\n",
        "   - **O que faz:** Controla o tamanho dos \"passos\" ao atualizar pesos da rede neural\n",
        "   - **Por que mudar:** 1e-5 é muito conservador para dataset pequeno\n",
        "   - **Impacto esperado:** Convergência mais rápida sem perder estabilidade\n",
        "   - **Alternativas:** Experimentar 5e-5 se 2e-5 não melhorar\n",
        "\n",
        "#### 2. **Dropout: 0.3 → 0.1** (3x menor)\n",
        "   - **O que faz:** \"Desliga\" aleatoriamente 10% dos neurônios durante treino\n",
        "   - **Por que mudar:** Dataset pequeno (~9K exemplos) + 30% dropout = underfitting\n",
        "   - **Impacto esperado:** Modelo consegue aprender melhor os padrões\n",
        "   - **Trade-off:** Menos regularização, mas dataset pequeno precisa de capacidade\n",
        "\n",
        "#### 3. **Movies Batch Size: 8 → 16** (2x maior)\n",
        "   - **O que faz:** Número de exemplos processados juntos antes de atualizar pesos\n",
        "   - **Por que mudar:** Batch maior = gradiente mais estável e representativo\n",
        "   - **Diferença de tags_batch_size:** \n",
        "     * movies (16): Tarefa complexa (diálogos → múltiplos filmes)\n",
        "     * tags (64): Tarefa simples (1 tag → 1 filme)\n",
        "   - **Limitação:** GPU precisa ter memória suficiente\n",
        "\n",
        "#### 4. **Num Epochs: 30 → 50**\n",
        "   - **O que faz:** Número de vezes que todo dataset é visto\n",
        "   - **Por que mudar:** Modelo precisa de mais tempo para aprender\n",
        "   - **Nota:** Artigo original usa 200 épocas (4x mais)\n",
        "\n",
        "#### 5. **num_movies: 6924 → None (automático)**\n",
        "   - **Problema original:** Valor hardcoded não correspondia ao dataset real\n",
        "   - **Solução:** Calculado automaticamente via `movie_mapper.get_num_movies()`\n",
        "   - **Impacto:** Evita bugs e garante dimensão correta da camada de saída"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Pu6zliCrzP1"
      },
      "source": [
        "## 3. Carregamento e Processamento dos Dados\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI3m0bgxTjSy"
      },
      "source": [
        "### 3.1 Dataset ReDial\n",
        "O dataset ReDial contém diálogos de recomendação de filmes entre um iniciador e um respondente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mvwZO-SEGIz"
      },
      "source": [
        "#### 3.1.1 Acesso ao dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5gmYyA8rzP1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Carregar dataset ReDial\n",
        "print(\"Carregando dataset ReDial...\")\n",
        "redial_dataset_raw = load_dataset('community-datasets/re_dial')\n",
        "print(f\"Train: {len(redial_dataset_raw['train'])} exemplos\")\n",
        "print(f\"Test: {len(redial_dataset_raw['test'])} exemplos\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Visualizar estrutura de um exemplo\n",
        "sample = redial_dataset_raw['train'][0]\n",
        "print(\"Estrutura de um exemplo:\")\n",
        "for key in sample.keys():\n",
        "    print(f\"  {key}: {type(sample[key])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V3QeabSrzP1"
      },
      "outputs": [],
      "source": [
        "class MovieIDMapper:\n",
        "    \"\"\"\n",
        "    Mapeia IDs de filmes entre diferentes formatos a partir do dataset ReDial. Exemplo: '@123' -> 123\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.movie_to_idx = {}\n",
        "        self.idx_to_movie = {}\n",
        "        self.movie_names = {}\n",
        "\n",
        "    def build_from_dataset(self, dataset):\n",
        "        \"\"\"Constrói mapeamento a partir do dataset ReDial\"\"\"\n",
        "        all_movies = set()\n",
        "\n",
        "        for split in ['train', 'test']:\n",
        "            for original_content in dataset[split]:\n",
        "                # Extrair IDs de filmes das mensagens\n",
        "                messages = original_content.get('messages', [])\n",
        "                for msg in messages:\n",
        "                    text = msg.get('text', '')\n",
        "                    movie_ids = re.findall(r'@(\\d+)', text)\n",
        "                    all_movies.update(movie_ids)\n",
        "\n",
        "                # Extrair dos movieMentions\n",
        "                mentions = original_content.get('movieMentions', {})\n",
        "                if isinstance(mentions, dict):\n",
        "                    for movie_id, name in mentions.items():\n",
        "                        all_movies.add(str(movie_id).replace('@', ''))\n",
        "                        self.movie_names[str(movie_id).replace('@', '')] = name\n",
        "\n",
        "        # Criar mapeamento ordenado\n",
        "        sorted_movies = sorted([int(m) for m in all_movies if m.isdigit()])\n",
        "\n",
        "        for idx, movie_id in enumerate(sorted_movies):\n",
        "            self.movie_to_idx[str(movie_id)] = idx\n",
        "            self.idx_to_movie[idx] = str(movie_id)\n",
        "\n",
        "        print(f\"Total de filmes únicos: {len(self.movie_to_idx)}\")\n",
        "        return self\n",
        "\n",
        "    def get_num_movies(self):\n",
        "        return len(self.movie_to_idx)\n",
        "\n",
        "    def movie_id_to_idx(self, movie_id):\n",
        "        movie_id = str(movie_id).replace('@', '')\n",
        "        return self.movie_to_idx.get(movie_id, -1)\n",
        "\n",
        "    def idx_to_movie_id(self, idx):\n",
        "        return self.idx_to_movie.get(idx, None)\n",
        "\n",
        "# Construir mapeamento\n",
        "movie_mapper = MovieIDMapper().build_from_dataset(redial_dataset_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brpc9myKrzP1"
      },
      "source": [
        "#### 3.1.2 Processamento dos Diálogos\n",
        "\n",
        "Conforme o artigo, concatenamos as utterances do iniciador com tokens [SEP] e usamos as recomendações do respondente como labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yTMSjZArzP1"
      },
      "outputs": [],
      "source": [
        "def process_dialogue(example, movie_mapper):\n",
        "    \"\"\"\n",
        "    Processa um diálogo do ReDial conforme descrito no artigo:\n",
        "    - Input: utterances do iniciador concatenadas com [SEP]\n",
        "    - Output: IDs dos filmes recomendados pelo respondente\n",
        "    - Movies mentioned: filmes mencionados pelo iniciador (para RNN)\n",
        "    \"\"\"\n",
        "    messages = example.get('messages', [])\n",
        "\n",
        "    initiator_texts = []\n",
        "    mentioned_movies = []  # Filmes mencionados pelo iniciador\n",
        "    recommended_movies = []  # Filmes recomendados pelo respondente\n",
        "\n",
        "    for msg in messages:\n",
        "        text = msg.get('text', '')\n",
        "        sender_id = msg.get('senderWorkerId', 0)\n",
        "\n",
        "        # Extrair IDs de filmes\n",
        "        movie_ids = re.findall(r'@(\\d+)', text)\n",
        "\n",
        "        # Determinar se é iniciador (primeiro sender) ou respondente\n",
        "        if sender_id == messages[0].get('senderWorkerId', 0):\n",
        "            # Iniciador - adicionar texto e filmes mencionados\n",
        "            # Substituir IDs por placeholder para versão sem RNN\n",
        "            clean_text = re.sub(r'@\\d+', '@', text)\n",
        "            initiator_texts.append(clean_text)\n",
        "            mentioned_movies.extend(movie_ids)\n",
        "        else:\n",
        "            # Respondente - coletar recomendações\n",
        "            recommended_movies.extend(movie_ids)\n",
        "\n",
        "    # Concatenar textos do iniciador com [SEP]\n",
        "    input_text = ' [SEP] '.join(initiator_texts)\n",
        "\n",
        "    # Converter IDs para índices\n",
        "    mentioned_indices = [movie_mapper.movie_id_to_idx(m) for m in mentioned_movies]\n",
        "    mentioned_indices = [idx for idx in mentioned_indices if idx >= 0]\n",
        "\n",
        "    recommended_indices = [movie_mapper.movie_id_to_idx(m) for m in recommended_movies]\n",
        "    recommended_indices = list(set([idx for idx in recommended_indices if idx >= 0]))\n",
        "\n",
        "    return {\n",
        "        'input_text': input_text,\n",
        "        'input_text_with_ids': ' [SEP] '.join([msg.get('text', '') for msg in messages\n",
        "                                               if msg.get('senderWorkerId') == messages[0].get('senderWorkerId')]),\n",
        "        'mentioned_movies': mentioned_indices,\n",
        "        'recommended_movies': recommended_indices\n",
        "    }\n",
        "\n",
        "# Processar dataset\n",
        "def process_split(dataset_split, movie_mapper):\n",
        "    processed = []\n",
        "    for example in tqdm(dataset_split, desc=\"Processando\"):\n",
        "        proc = process_dialogue(example, movie_mapper)\n",
        "        # Filtrar exemplos sem recomendações\n",
        "        if proc['recommended_movies'] and proc['input_text'].strip():\n",
        "            processed.append(proc)\n",
        "    return processed\n",
        "\n",
        "print(\"Processando split de treino...\")\n",
        "train_data = process_split(redial_dataset_raw['train'], movie_mapper)\n",
        "print(f\"Exemplos de treino válidos: {len(train_data)}\")\n",
        "\n",
        "print(\"\\nProcessando split de teste...\")\n",
        "test_data = process_split(redial_dataset_raw['test'], movie_mapper)\n",
        "print(f\"Exemplos de teste válidos: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jdiP4qAWrzP2"
      },
      "outputs": [],
      "source": [
        "# Visualizar exemplo processado\n",
        "print(\"Exemplo processado:\")\n",
        "print(f\"Input text: {train_data[0]['input_text'][:500]}...\")\n",
        "print(f\"\\nMentioned movies (índices): {train_data[0]['mentioned_movies'][:5]}\")\n",
        "print(f\"Recommended movies (índices): {train_data[0]['recommended_movies']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EOQYxyVrzP2"
      },
      "source": [
        "### 3.2 Dataset MovieLens\n",
        "\n",
        "Para o experimento de multi-task learning, usamos tags de usuários do MovieLens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ-Adm7vrzP2"
      },
      "outputs": [],
      "source": [
        "# Download MovieLens tags\n",
        "!wget -q -nc http://files.grouplens.org/datasets/movielens/ml-latest.zip\n",
        "!unzip -q -o ml-latest.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PLyP7aa0rzP2"
      },
      "outputs": [],
      "source": [
        "# Carregar dados do MovieLens\n",
        "tags_csv = pd.read_csv('ml-latest/tags.csv')\n",
        "movies_csv = pd.read_csv('ml-latest/movies.csv')\n",
        "\n",
        "print(f\"Tags totais: {len(tags_csv)}\")\n",
        "print(f\"Filmes totais: {len(movies_csv)}\")\n",
        "print(f\"\\nExemplo de tags:\")\n",
        "print(tags_csv.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zbQ8_hmHrzP2"
      },
      "outputs": [],
      "source": [
        "def create_tag_dataset(tags_csv, movie_mapper, max_tags_per_movie=50):\n",
        "    \"\"\"\n",
        "    Cria dataset de tags para multi-task learning\n",
        "    Input: tag text\n",
        "    Output: movie index\n",
        "    \"\"\"\n",
        "    tag_data = []\n",
        "\n",
        "    # Agrupar tags por filme\n",
        "    for movie_id, group in tags_csv.groupby('movieId'):\n",
        "        movie_idx = movie_mapper.movie_id_to_idx(str(movie_id))\n",
        "        if movie_idx < 0:\n",
        "            continue\n",
        "\n",
        "        tags = group['tag'].tolist()[:max_tags_per_movie]\n",
        "        for tag in tags:\n",
        "            if isinstance(tag, str) and len(tag.strip()) > 2:\n",
        "                tag_data.append({\n",
        "                    'tag_text': tag.strip(),\n",
        "                    'movie_idx': movie_idx\n",
        "                })\n",
        "\n",
        "    return tag_data\n",
        "\n",
        "# Criar dataset de tags (movie_mapper definido junto do dataset do redial)\n",
        "tag_data = create_tag_dataset(tags_csv, movie_mapper)\n",
        "print(f\"Exemplos de tags: {len(tag_data)}\")\n",
        "print(f\"\\nExemplo de tag:\")\n",
        "print(tag_data[0])\n",
        "print(\"/n\")\n",
        "\n",
        "# Split treino/teste para tags\n",
        "random.shuffle(tag_data)\n",
        "split_idx = int(len(tag_data) * 0.9)\n",
        "tag_train_data = tag_data[:split_idx]\n",
        "tag_test_data = tag_data[split_idx:]\n",
        "\n",
        "print(f\"Tags treino: {len(tag_train_data)}\")\n",
        "print(f\"Tags teste: {len(tag_test_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-LaEGBNrzP2"
      },
      "source": [
        "## 4. Dataset Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JNQKLu1rzP2"
      },
      "outputs": [],
      "source": [
        "class MovieRecommendationDataset(Dataset):\n",
        "    \"\"\"Dataset para recomendação de filmes (tarefa principal)\"\"\"\n",
        "\n",
        "    def __init__(self, data, tokenizer, num_movies, max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.num_movies = num_movies\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        # Tokenizar input\n",
        "        encoding = self.tokenizer(\n",
        "            item['input_text'],\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Criar label multi-hot\n",
        "        labels = torch.zeros(self.num_movies)\n",
        "        for movie_idx in item['recommended_movies']:\n",
        "            if 0 <= movie_idx < self.num_movies:\n",
        "                labels[movie_idx] = 1.0\n",
        "\n",
        "        # Filmes mencionados (para RNN)\n",
        "        mentioned = item['mentioned_movies'][:20]  # Limitar\n",
        "        mentioned_tensor = torch.zeros(20, dtype=torch.long)\n",
        "        mentioned_mask = torch.zeros(20, dtype=torch.bool)\n",
        "\n",
        "        for i, m_idx in enumerate(mentioned):\n",
        "            if i < 20 and 0 <= m_idx < self.num_movies:\n",
        "                mentioned_tensor[i] = m_idx\n",
        "                mentioned_mask[i] = True\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'labels': labels,\n",
        "            'mentioned_movies': mentioned_tensor,\n",
        "            'mentioned_mask': mentioned_mask\n",
        "        }\n",
        "\n",
        "\n",
        "class TagDataset(Dataset):\n",
        "    \"\"\"Dataset para predição de filme a partir de tag (tarefa auxiliar)\"\"\"\n",
        "\n",
        "    def __init__(self, data, tokenizer, max_length=64):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            item['tag_text'],\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(item['movie_idx'], dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYnXsEbcrzP2"
      },
      "outputs": [],
      "source": [
        "# Inicializar tokenizer para SBERT\n",
        "sbert_tokenizer = AutoTokenizer.from_pretrained(config.sbert_model_name)\n",
        "print(f\"Cria o tokenizador do Modelo: {config.sbert_model_name}\")\n",
        "\n",
        "# Atualizar número de filmes baseado no mapeamento real\n",
        "config.num_movies = movie_mapper.get_num_movies()\n",
        "print(f\"Número de filmes: {config.num_movies}\")\n",
        "\n",
        "# Criar datasets\n",
        "train_dataset = MovieRecommendationDataset(\n",
        "    train_data, sbert_tokenizer, config.num_movies, config.max_seq_length\n",
        ")\n",
        "test_dataset = MovieRecommendationDataset(\n",
        "    test_data, sbert_tokenizer, config.num_movies, config.max_seq_length\n",
        ")\n",
        "\n",
        "tag_train_dataset = TagDataset(tag_train_data, sbert_tokenizer)\n",
        "tag_test_dataset = TagDataset(tag_test_data, sbert_tokenizer)\n",
        "\n",
        "print(f\"\\nDataset de treino: {len(train_dataset)} exemplos\")\n",
        "print(f\"Dataset de teste: {len(test_dataset)} exemplos\")\n",
        "print(f\"Dataset de tags treino: {len(tag_train_dataset)} exemplos\")\n",
        "\n",
        "# Diagnóstico de configuração\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DIAGNÓSTICO DE TREINAMENTO\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Batches por época (movies): {len(train_dataset) // config.movies_batch_size}\")\n",
        "print(f\"Total de steps (épocas): {(len(train_dataset) // config.movies_batch_size) * config.num_epochs}\")\n",
        "print(f\"Warmup steps (10%): {int((len(train_dataset) // config.movies_batch_size) * config.num_epochs * 0.1)}\")\n",
        "print(f\"\\nEspaço de classificação: {config.num_movies} filmes possíveis\")\n",
        "print(f\"Média de labels positivos por exemplo: {sum(len(d['recommended_movies']) for d in train_data) / len(train_data):.2f}\")\n",
        "print(f\"Taxa de desbalanceamento: {config.num_movies / (sum(len(d['recommended_movies']) for d in train_data) / len(train_data)):.1f}:1\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwkJ0yNgrzP2"
      },
      "source": [
        "## 5. Arquitetura dos Modelos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri8m01PDTVpi"
      },
      "source": [
        "### 5.1 Modelo Baseline: SBERT + FFN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V0YwgyIrzP2"
      },
      "outputs": [],
      "source": [
        "class SBERTMovieRecommender(nn.Module):\n",
        "    \"\"\"\n",
        "    Modelo baseline: SBERT + FFN para classificação multi-label.\n",
        "\n",
        "    f(U) = FFN(SBERT_mean_pooling(U))\n",
        "\n",
        "    onde U é o input concatenado com [SEP] tokens.\n",
        "    SBERT usa mean pooling dos tokens ao invés de apenas [CLS].\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # Carregar modelo SBERT base\n",
        "        self.sbert = AutoModel.from_pretrained(config.sbert_model_name)\n",
        "\n",
        "        # FFN para projeção\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(config.dropout_prob),\n",
        "            nn.Linear(config.sbert_hidden_size, config.ffn_hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.dropout_prob),\n",
        "            nn.Linear(config.ffn_hidden_size, config.num_movies)\n",
        "        )\n",
        "\n",
        "    def mean_pooling(self, token_embeddings, attention_mask):\n",
        "        \"\"\"Mean pooling - considera attention mask para média correta\"\"\"\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "        return sum_embeddings / sum_mask\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, **kwargs):\n",
        "        # Encoding SBERT\n",
        "        outputs = self.sbert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        # Mean pooling (característica do SBERT)\n",
        "        sentence_embeddings = self.mean_pooling(outputs.last_hidden_state, attention_mask)\n",
        "\n",
        "        # Projeção para logits\n",
        "        logits = self.classifier(sentence_embeddings)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def get_sentence_embedding(self, input_ids, attention_mask):\n",
        "        \"\"\"Retorna sentence embedding para multi-task\"\"\"\n",
        "        outputs = self.sbert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        return self.mean_pooling(outputs.last_hidden_state, attention_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcbblvYnrzP2"
      },
      "source": [
        "### 5.2 Modelo com RNN para Features Colaborativas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OucrIy1irzP2"
      },
      "outputs": [],
      "source": [
        "class SBERTRNNMovieRecommender(nn.Module):\n",
        "    \"\"\"\n",
        "    Modelo com RNN para aprender features colaborativas.\n",
        "\n",
        "    f(U) = FFN(SBERT_mean_pooling(U), RNN(L(U)))\n",
        "\n",
        "    onde L(U) é a lista de filmes mencionados em U.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.sbert = AutoModel.from_pretrained(config.sbert_model_name)\n",
        "\n",
        "        # Embedding de filmes para RNN\n",
        "        self.movie_embedding = nn.Embedding(\n",
        "            config.num_movies + 1,  # +1 para padding\n",
        "            config.rnn_embedding_size,\n",
        "            padding_idx=config.num_movies\n",
        "        )\n",
        "\n",
        "        # RNN para processar sequência de filmes mencionados\n",
        "        self.rnn = nn.GRU(\n",
        "            input_size=config.rnn_embedding_size,\n",
        "            hidden_size=config.rnn_hidden_size,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Dimensão combinada: SBERT embedding + RNN output (bidirectional)\n",
        "        combined_size = config.sbert_hidden_size + (config.rnn_hidden_size * 2)\n",
        "\n",
        "        # FFN para projeção\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(config.dropout_prob),\n",
        "            nn.Linear(combined_size, config.ffn_hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.dropout_prob),\n",
        "            nn.Linear(config.ffn_hidden_size, config.num_movies)\n",
        "        )\n",
        "\n",
        "        self.num_movies = config.num_movies\n",
        "\n",
        "    def mean_pooling(self, token_embeddings, attention_mask):\n",
        "        \"\"\"Mean pooling - considera attention mask para média correta\"\"\"\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "        return sum_embeddings / sum_mask\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, mentioned_movies, mentioned_mask, **kwargs):\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        # Encoding SBERT\n",
        "        sbert_outputs = self.sbert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        sentence_embeddings = self.mean_pooling(sbert_outputs.last_hidden_state, attention_mask)\n",
        "\n",
        "        # Processar filmes mencionados com RNN\n",
        "        # Substituir índices inválidos pelo índice de padding\n",
        "        mentioned_movies = mentioned_movies.clone()\n",
        "        mentioned_movies[~mentioned_mask] = self.num_movies\n",
        "\n",
        "        movie_embeds = self.movie_embedding(mentioned_movies)\n",
        "\n",
        "        # RNN\n",
        "        rnn_output, hidden = self.rnn(movie_embeds)\n",
        "\n",
        "        # Usar último hidden state (concatenado de ambas direções)\n",
        "        rnn_features = hidden.transpose(0, 1).contiguous().view(batch_size, -1)\n",
        "\n",
        "        # Combinar features\n",
        "        combined = torch.cat([sentence_embeddings, rnn_features], dim=-1)\n",
        "\n",
        "        # Projeção para logits\n",
        "        logits = self.classifier(combined)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def get_sentence_embedding(self, input_ids, attention_mask):\n",
        "        \"\"\"Retorna sentence embedding para multi-task\"\"\"\n",
        "        outputs = self.sbert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        return self.mean_pooling(outputs.last_hidden_state, attention_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKyimV0KrzP3"
      },
      "source": [
        "### 5.3 Wrapper para Multi-Task Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mn7QJfChrzP3"
      },
      "outputs": [],
      "source": [
        "class MultiTaskWrapper(nn.Module):\n",
        "    \"\"\"\n",
        "    Wrapper para multi-task learning com tarefa auxiliar de tags.\n",
        "\n",
        "    Loss = BCE(f(U), y) + CE(f(U), z)\n",
        "\n",
        "    onde z é o filme correto para uma tag.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_model, config):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "\n",
        "        # Head separado para tarefa de tags (usa mesmo pathway do SBERT)\n",
        "        self.tag_classifier = nn.Sequential(\n",
        "            nn.Dropout(config.dropout_prob),\n",
        "            nn.Linear(config.sbert_hidden_size, config.ffn_hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.dropout_prob),\n",
        "            nn.Linear(config.ffn_hidden_size, config.num_movies)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, **kwargs):\n",
        "        return self.base_model(input_ids, attention_mask, **kwargs)\n",
        "\n",
        "    def forward_tags(self, input_ids, attention_mask):\n",
        "        \"\"\"Forward pass para tarefa de tags\"\"\"\n",
        "        # Obter sentence embedding do modelo base\n",
        "        if hasattr(self.base_model, 'get_sentence_embedding'):\n",
        "            embeddings = self.base_model.get_sentence_embedding(input_ids, attention_mask)\n",
        "        else:\n",
        "            embeddings = self.base_model.get_sentence_embedding(input_ids, attention_mask)\n",
        "\n",
        "        logits = self.tag_classifier(embeddings)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0egapTHcrzP3"
      },
      "source": [
        "## 6. Métricas de Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIqgo5V4rzP3"
      },
      "outputs": [],
      "source": [
        "def compute_ndcg_at_k(predictions, labels, k=10):\n",
        "    \"\"\"\n",
        "    Calcula nDCG@k conforme usado no artigo.\n",
        "\n",
        "    Args:\n",
        "        predictions: tensor de logits (batch_size, num_movies)\n",
        "        labels: tensor multi-hot de labels (batch_size, num_movies)\n",
        "        k: número de itens para considerar\n",
        "\n",
        "    Returns:\n",
        "        nDCG@k médio\n",
        "    \"\"\"\n",
        "    predictions = predictions.detach().cpu().numpy()\n",
        "    labels = labels.detach().cpu().numpy()\n",
        "\n",
        "    ndcg_scores = []\n",
        "\n",
        "    for pred, label in zip(predictions, labels):\n",
        "        # Se não há labels positivos, pular\n",
        "        if label.sum() == 0:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            score = ndcg_score([label], [pred], k=k)\n",
        "            ndcg_scores.append(score)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    return np.mean(ndcg_scores) if ndcg_scores else 0.0\n",
        "\n",
        "\n",
        "def compute_recall_at_k(predictions, labels, k=10):\n",
        "    \"\"\"Calcula Recall@k\"\"\"\n",
        "    predictions = predictions.detach().cpu().numpy()\n",
        "    labels = labels.detach().cpu().numpy()\n",
        "\n",
        "    recalls = []\n",
        "\n",
        "    for pred, label in zip(predictions, labels):\n",
        "        if label.sum() == 0:\n",
        "            continue\n",
        "\n",
        "        top_k_indices = np.argsort(pred)[-k:]\n",
        "        relevant = label[top_k_indices].sum()\n",
        "        total_relevant = label.sum()\n",
        "\n",
        "        recalls.append(relevant / total_relevant)\n",
        "\n",
        "    return np.mean(recalls) if recalls else 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuC_leOxrzP3"
      },
      "source": [
        "## 7. Loop de Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKqrpZZPrzP3"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    \"\"\"Trainer para os modelos de recomendação\"\"\"\n",
        "\n",
        "    def __init__(self, model, config, train_loader, eval_loader,\n",
        "                 tag_train_loader=None, tag_eval_loader=None,\n",
        "                 use_multitask=False):\n",
        "        self.model = model.to(device)\n",
        "        self.config = config\n",
        "        self.train_loader = train_loader\n",
        "        self.eval_loader = eval_loader\n",
        "        self.tag_train_loader = tag_train_loader\n",
        "        self.tag_eval_loader = tag_eval_loader\n",
        "        self.use_multitask = use_multitask\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = AdamW(\n",
        "            model.parameters(),\n",
        "            lr=config.learning_rate\n",
        "        )\n",
        "\n",
        "        # Scheduler\n",
        "        total_steps = len(train_loader) * config.num_epochs\n",
        "        warmup_steps = int(total_steps * config.warmup_ratio)\n",
        "\n",
        "        self.scheduler = get_linear_schedule_with_warmup(\n",
        "            self.optimizer,\n",
        "            num_warmup_steps=warmup_steps,\n",
        "            num_training_steps=total_steps\n",
        "        )\n",
        "\n",
        "        # ✅ Calcular pos_weight para balancear classes\n",
        "        print(\"\\nCalculando pos_weight para balancear classes...\")\n",
        "\n",
        "        sample_labels = []\n",
        "        num_batches_to_sample = min(100, len(train_loader))\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            sample_labels.append(batch['labels'])\n",
        "            if i >= num_batches_to_sample - 1:\n",
        "                break\n",
        "\n",
        "        sample_labels = torch.cat(sample_labels, dim=0)\n",
        "\n",
        "        num_positives = sample_labels.sum()\n",
        "        num_negatives = sample_labels.numel() - num_positives\n",
        "        pos_weight_value = (num_negatives / num_positives).item()\n",
        "\n",
        "        print(f\"  • Amostras analisadas: {len(sample_labels):,}\")\n",
        "        print(f\"  • Labels positivos: {num_positives.item():,}\")\n",
        "        print(f\"  • Labels negativos: {num_negatives.item():,}\")\n",
        "        print(f\"  • Taxa de positivos: {num_positives/sample_labels.numel()*100:.4f}%\")\n",
        "        print(f\"  • pos_weight calculado: {pos_weight_value:.1f}\")\n",
        "\n",
        "        # Loss functions\n",
        "        self.bce_loss = nn.BCEWithLogitsLoss(\n",
        "            pos_weight=torch.full((config.num_movies,), pos_weight_value).to(device)\n",
        "        )\n",
        "\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Histórico\n",
        "        self.history = {\n",
        "            'train_loss': [],\n",
        "            'eval_loss': [],\n",
        "            'ndcg': [],\n",
        "            'recall': []\n",
        "        }\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        # Iterator para tags se multi-task\n",
        "        tag_iter = iter(self.tag_train_loader) if self.use_multitask and self.tag_train_loader else None\n",
        "\n",
        "        progress_bar = tqdm(self.train_loader, desc=\"Training\")\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            # Move batch para device\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass principal\n",
        "            logits = self.model(\n",
        "                input_ids=batch['input_ids'],\n",
        "                attention_mask=batch['attention_mask'],\n",
        "                mentioned_movies=batch.get('mentioned_movies'),\n",
        "                mentioned_mask=batch.get('mentioned_mask')\n",
        "            )\n",
        "\n",
        "            # Loss principal (BCE multi-label)\n",
        "            loss = self.bce_loss(logits, batch['labels'])\n",
        "\n",
        "            # Multi-task: adicionar loss de tags\n",
        "            if self.use_multitask and tag_iter:\n",
        "                try:\n",
        "                    tag_batch = next(tag_iter)\n",
        "                except StopIteration:\n",
        "                    tag_iter = iter(self.tag_train_loader)\n",
        "                    tag_batch = next(tag_iter)\n",
        "\n",
        "                tag_batch = {k: v.to(device) for k, v in tag_batch.items()}\n",
        "\n",
        "                tag_logits = self.model.forward_tags(\n",
        "                    input_ids=tag_batch['input_ids'],\n",
        "                    attention_mask=tag_batch['attention_mask']\n",
        "                )\n",
        "\n",
        "                tag_loss = self.ce_loss(tag_logits, tag_batch['label'])\n",
        "                loss = loss + tag_loss  # Peso igual conforme artigo\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "\n",
        "            self.optimizer.step()\n",
        "            self.scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        return total_loss / num_batches\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "\n",
        "        for batch in tqdm(self.eval_loader, desc=\"Evaluating\"):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            logits = self.model(\n",
        "                input_ids=batch['input_ids'],\n",
        "                attention_mask=batch['attention_mask'],\n",
        "                mentioned_movies=batch.get('mentioned_movies'),\n",
        "                mentioned_mask=batch.get('mentioned_mask')\n",
        "            )\n",
        "\n",
        "            loss = self.bce_loss(logits, batch['labels'])\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            all_predictions.append(logits)\n",
        "            all_labels.append(batch['labels'])\n",
        "\n",
        "        # Concatenar todas as predições\n",
        "        all_predictions = torch.cat(all_predictions, dim=0)\n",
        "        all_labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "        # Calcular métricas\n",
        "        ndcg = compute_ndcg_at_k(all_predictions, all_labels, k=self.config.eval_k)\n",
        "        recall = compute_recall_at_k(all_predictions, all_labels, k=self.config.eval_k)\n",
        "\n",
        "        return {\n",
        "            'loss': total_loss / len(self.eval_loader),\n",
        "            'ndcg@10': ndcg,\n",
        "            'recall@10': recall\n",
        "        }\n",
        "\n",
        "    def train(self, num_epochs=None):\n",
        "        num_epochs = num_epochs or self.config.num_epochs\n",
        "        best_ndcg = 0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"\\n{'='*50}\")\n",
        "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "            print('='*50)\n",
        "\n",
        "            # Treinar\n",
        "            train_loss = self.train_epoch()\n",
        "            self.history['train_loss'].append(train_loss)\n",
        "\n",
        "            # Avaliar\n",
        "            eval_metrics = self.evaluate()\n",
        "            self.history['eval_loss'].append(eval_metrics['loss'])\n",
        "            self.history['ndcg'].append(eval_metrics['ndcg@10'])\n",
        "            self.history['recall'].append(eval_metrics['recall@10'])\n",
        "\n",
        "            print(f\"\\nTrain Loss: {train_loss:.4f}\")\n",
        "            print(f\"Eval Loss: {eval_metrics['loss']:.4f}\")\n",
        "            print(f\"nDCG@10: {eval_metrics['ndcg@10']:.4f}\")\n",
        "            print(f\"Recall@10: {eval_metrics['recall@10']:.4f}\")\n",
        "\n",
        "            # Salvar melhor modelo\n",
        "            if eval_metrics['ndcg@10'] > best_ndcg:\n",
        "                best_ndcg = eval_metrics['ndcg@10']\n",
        "                torch.save(\n",
        "                    self.model.state_dict(),\n",
        "                    os.path.join(self.config.save_dir, 'best_model.pt')\n",
        "                )\n",
        "                print(f\"Novo melhor modelo salvo! nDCG@10: {best_ndcg:.4f}\")\n",
        "\n",
        "        return self.history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNds_WmjrzP3"
      },
      "source": [
        "## 8. Experimento 1: Baseline SBERT + FFN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGr2PuvzrzP3"
      },
      "outputs": [],
      "source": [
        "# Criar DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config.movies_batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,  # 0 para evitar problemas com multiprocessing no Colab\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "eval_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config.movies_batch_size * 2,\n",
        "    shuffle=False,\n",
        "    num_workers=0,  # 0 para evitar problemas com multiprocessing no Colab\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"Batches de treino: {len(train_loader)}\")\n",
        "print(f\"Batches de avaliação: {len(eval_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "794rVjELrzP3"
      },
      "outputs": [],
      "source": [
        "# Treinar modelo baseline\n",
        "print(\"=\"*60)\n",
        "print(\"EXPERIMENTO 1: SBERT Baseline (sem RNN, sem multi-task)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "baseline_model = SBERTMovieRecommender(config)\n",
        "\n",
        "baseline_trainer = Trainer(\n",
        "    model=baseline_model,\n",
        "    config=config,\n",
        "    train_loader=train_loader,\n",
        "    eval_loader=eval_loader,\n",
        "    use_multitask=False\n",
        ")\n",
        "\n",
        "# Treinar (reduzido para demonstração)\n",
        "baseline_history = baseline_trainer.train(config.num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukQ0vpS_rzP3"
      },
      "source": [
        "## 9. Experimento 2: SBERT + RNN para Features Colaborativas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5hgqNT3erzP3"
      },
      "outputs": [],
      "source": [
        "# Treinar modelo com RNN\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENTO 2: SBERT + RNN (features colaborativas)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "rnn_model = SBERTRNNMovieRecommender(config)\n",
        "\n",
        "rnn_trainer = Trainer(\n",
        "    model=rnn_model,\n",
        "    config=config,\n",
        "    train_loader=train_loader,\n",
        "    eval_loader=eval_loader,\n",
        "    use_multitask=False\n",
        ")\n",
        "\n",
        "rnn_history = rnn_trainer.train(config.num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld8boIMerzP3"
      },
      "source": [
        "## 10. Experimento 3: Multi-Task Learning com Tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUU7DpZirzP3"
      },
      "outputs": [],
      "source": [
        "# Criar DataLoaders para tags\n",
        "tag_train_loader = DataLoader(\n",
        "    tag_train_dataset,\n",
        "    batch_size=config.tags_batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,  # 0 para evitar problemas com multiprocessing no Colab\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "tag_eval_loader = DataLoader(\n",
        "    tag_test_dataset,\n",
        "    batch_size=config.tags_batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=0,  # 0 para evitar problemas com multiprocessing no Colab\n",
        "    pin_memory=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plrkpj03rzP3"
      },
      "outputs": [],
      "source": [
        "# Treinar modelo com multi-task (SBERT baseline + tags)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENTO 3: SBERT + Multi-Task (user tags)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "base_model_mt = SBERTMovieRecommender(config)\n",
        "multitask_model = MultiTaskWrapper(base_model_mt, config)\n",
        "\n",
        "multitask_trainer = Trainer(\n",
        "    model=multitask_model,\n",
        "    config=config,\n",
        "    train_loader=train_loader,\n",
        "    eval_loader=eval_loader,\n",
        "    tag_train_loader=tag_train_loader,\n",
        "    tag_eval_loader=tag_eval_loader,\n",
        "    use_multitask=True\n",
        ")\n",
        "\n",
        "multitask_history = multitask_trainer.train(config.num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaClvCTtrzP4"
      },
      "source": [
        "## 11. Experimento 4: SBERT + RNN + Multi-Task (Modelo Completo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "f2AYmZiTrzP4"
      },
      "outputs": [],
      "source": [
        "# Modelo completo: RNN + Multi-task\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENTO 4: SBERT + RNN + Multi-Task (modelo completo)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "rnn_base_model = SBERTRNNMovieRecommender(config)\n",
        "full_model = MultiTaskWrapper(rnn_base_model, config)\n",
        "\n",
        "full_trainer = Trainer(\n",
        "    model=full_model,\n",
        "    config=config,\n",
        "    train_loader=train_loader,\n",
        "    eval_loader=eval_loader,\n",
        "    tag_train_loader=tag_train_loader,\n",
        "    tag_eval_loader=tag_eval_loader,\n",
        "    use_multitask=True\n",
        ")\n",
        "\n",
        "full_history = full_trainer.train(config.num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXJvrCE4rzP4"
      },
      "source": [
        "## 12. Comparação dos Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ty2TGCWrzP4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_results(histories, names):\n",
        "    \"\"\"Plota comparação dos resultados\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "    colors = ['#2ecc71', '#3498db', '#e74c3c', '#9b59b6']\n",
        "\n",
        "    # Train Loss\n",
        "    ax = axes[0, 0]\n",
        "    for hist, name, color in zip(histories, names, colors):\n",
        "        ax.plot(hist['train_loss'], label=name, color=color, linewidth=2)\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.set_title('Training Loss')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Eval Loss\n",
        "    ax = axes[0, 1]\n",
        "    for hist, name, color in zip(histories, names, colors):\n",
        "        ax.plot(hist['eval_loss'], label=name, color=color, linewidth=2)\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.set_title('Evaluation Loss')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # nDCG@10\n",
        "    ax = axes[1, 0]\n",
        "    for hist, name, color in zip(histories, names, colors):\n",
        "        ax.plot(hist['ndcg'], label=name, color=color, linewidth=2)\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('nDCG@10')\n",
        "    ax.set_title('nDCG@10 (métrica principal do artigo)')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Recall@10\n",
        "    ax = axes[1, 1]\n",
        "    for hist, name, color in zip(histories, names, colors):\n",
        "        ax.plot(hist['recall'], label=name, color=color, linewidth=2)\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Recall@10')\n",
        "    ax.set_title('Recall@10')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_results.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Plotar resultados\n",
        "plot_results(\n",
        "    [baseline_history, rnn_history, multitask_history, full_history],\n",
        "    ['SBERT Baseline', 'SBERT + RNN', 'SBERT + Multi-Task', 'SBERT + RNN + Multi-Task']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lj9NYs_jrzP4"
      },
      "outputs": [],
      "source": [
        "\n",
        "results = {\n",
        "    'Modelo': [\n",
        "        'SBERT Baseline',\n",
        "        'SBERT + RNN',\n",
        "        'SBERT + Multi-Task',\n",
        "        'SBERT + RNN + Multi-Task',\n",
        "        '--- Artigo Original (BERT) ---',\n",
        "        'Artigo: Sem RNN, Sem Tags',\n",
        "        'Artigo: Com RNN, Sem Tags',\n",
        "        'Artigo: Sem RNN, Com Tags',\n",
        "        'Artigo: Com RNN, Com Tags'\n",
        "    ],\n",
        "    'nDCG@10': [\n",
        "        max(baseline_history['ndcg']),\n",
        "        max(rnn_history['ndcg']),\n",
        "        max(multitask_history['ndcg']),\n",
        "        max(full_history['ndcg']),\n",
        "        '-',\n",
        "        0.130,\n",
        "        0.165,\n",
        "        0.138,\n",
        "        0.169\n",
        "    ]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Nota: O artigo original usa BERT e reporta nDCG@10 de 0.819 para\")\n",
        "print(\"modelos conversacionais completos (tarefa diferente). Esta implementação\")\n",
        "print(\"usa SBERT e foca na tarefa de recomendação one-shot a partir de queries.\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlbxdCJtrzP4"
      },
      "source": [
        "## 13. Inferência e Demonstração"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcnAWh-VrzP4"
      },
      "outputs": [],
      "source": [
        "class MovieRecommenderInference:\n",
        "    \"\"\"Classe para inferência com o modelo treinado\"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer, movie_mapper, device, top_k=10):\n",
        "        self.model = model.to(device)\n",
        "        self.model.eval()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.movie_mapper = movie_mapper\n",
        "        self.device = device\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def recommend(self, query, mentioned_movie_names=None):\n",
        "        \"\"\"\n",
        "        Gera recomendações a partir de uma query.\n",
        "\n",
        "        Args:\n",
        "            query: texto da query do usuário\n",
        "            mentioned_movie_names: lista de nomes de filmes mencionados (opcional)\n",
        "\n",
        "        Returns:\n",
        "            Lista de filmes recomendados com scores\n",
        "        \"\"\"\n",
        "        # Tokenizar\n",
        "        encoding = self.tokenizer(\n",
        "            query,\n",
        "            max_length=512,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].to(self.device)\n",
        "        attention_mask = encoding['attention_mask'].to(self.device)\n",
        "\n",
        "        # Preparar filmes mencionados (placeholder se não disponível)\n",
        "        mentioned_movies = torch.zeros(1, 20, dtype=torch.long, device=self.device)\n",
        "        mentioned_mask = torch.zeros(1, 20, dtype=torch.bool, device=self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                mentioned_movies=mentioned_movies,\n",
        "                mentioned_mask=mentioned_mask\n",
        "            )\n",
        "\n",
        "        # Obter top-k\n",
        "        probs = torch.sigmoid(logits).squeeze(0)\n",
        "        top_scores, top_indices = torch.topk(probs, self.top_k)\n",
        "\n",
        "        recommendations = []\n",
        "        for score, idx in zip(top_scores.cpu().numpy(), top_indices.cpu().numpy()):\n",
        "            movie_id = self.movie_mapper.idx_to_movie_id(idx)\n",
        "            movie_name = self.movie_mapper.movie_names.get(movie_id, f\"Movie {movie_id}\")\n",
        "            recommendations.append({\n",
        "                'movie_id': movie_id,\n",
        "                'name': movie_name,\n",
        "                'score': float(score)\n",
        "            })\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "# Criar inference engine com o melhor modelo\n",
        "inference = MovieRecommenderInference(\n",
        "    model=full_model,\n",
        "    tokenizer=sbert_tokenizer,\n",
        "    movie_mapper=movie_mapper,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKPUKA1trzP4"
      },
      "outputs": [],
      "source": [
        "# Demonstração de inferência\n",
        "print(\"=\"*60)\n",
        "print(\"DEMONSTRAÇÃO DE RECOMENDAÇÕES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_queries = [\n",
        "    \"I like animations and comedies. I enjoyed Toy Story and Finding Nemo.\",\n",
        "    \"I'm looking for something dramatic and artistic. I love Christopher Nolan films.\",\n",
        "    \"Can you recommend some action movies? I like Marvel superhero films.\",\n",
        "    \"I want to watch something scary for Halloween. Horror movies please!\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"\\n{'─'*60}\")\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"{'─'*60}\")\n",
        "\n",
        "    recommendations = inference.recommend(query)\n",
        "\n",
        "    print(\"\\nTop 5 Recomendações:\")\n",
        "    for i, rec in enumerate(recommendations[:5], 1):\n",
        "        print(f\"  {i}. {rec['name']} (score: {rec['score']:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTK_FbpLrzP5"
      },
      "source": [
        "## 14. Análise de Erros e Limitações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzccv0NhrzP5"
      },
      "outputs": [],
      "source": [
        "# Análise conforme discutido no artigo\n",
        "print(\"=\"*70)\n",
        "print(\"ANÁLISE DE LIMITAÇÕES (conforme artigo)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "analysis = \"\"\"\n",
        "1. TAMANHO DO DATASET\n",
        "   - Treino: {} exemplos\n",
        "   - Teste: {} exemplos\n",
        "   - O artigo menciona ~8008 treino e ~2002 avaliação\n",
        "   - Dataset pequeno leva a overfitting\n",
        "\n",
        "2. QUALIDADE DOS DADOS\n",
        "   - Sentenças concatenadas de diálogos podem não ser significativas\n",
        "   - Exemplo: \"Anything artistic [SEP] What's it about?\" não faz sentido isolado\n",
        "\n",
        "3. COBERTURA DE FILMES\n",
        "   - Total de filmes no mapeamento: {}\n",
        "   - Nem todos têm tags de usuários para multi-task\n",
        "\n",
        "4. COMPARAÇÃO COM ARTIGO\n",
        "   - Artigo reporta nDCG@10 entre 0.130 e 0.169\n",
        "   - Modelos conversacionais completos atingem 0.819\n",
        "   - Nossa tarefa é mais difícil (one-shot vs conversacional)\n",
        "\n",
        "5. MELHORIAS OBSERVADAS\n",
        "   - RNN para features colaborativas: +0.035 nDCG (artigo)\n",
        "   - Multi-task com tags: +0.004-0.008 nDCG (artigo)\n",
        "\"\"\".format(\n",
        "    len(train_data),\n",
        "    len(test_data),\n",
        "    movie_mapper.get_num_movies()\n",
        ")\n",
        "\n",
        "print(analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AX283pfrzP5"
      },
      "source": [
        "## 15. Salvar Modelo Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GFXCD--rzP5"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Salvar modelo completo e configurações\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "class NumpyEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, (np.floating, np.integer)):\n",
        "            return obj.item()\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        elif hasattr(obj, 'item'):  # Para tensores PyTorch\n",
        "            return obj.item()\n",
        "        return super().default(obj)\n",
        "\n",
        "save_path = os.path.join(config.save_dir, 'final_model')\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Salvar pesos do modelo\n",
        "torch.save(full_model.state_dict(), os.path.join(save_path, 'model_weights.pt'))\n",
        "\n",
        "# Salvar configurações\n",
        "config_dict = {k: v for k, v in vars(config).items() if not k.startswith('_')}\n",
        "with open(os.path.join(save_path, 'config.json'), 'w') as f:\n",
        "    json.dump(config_dict, f, indent=2, cls=NumpyEncoder)\n",
        "\n",
        "# Salvar mapeamento de filmes\n",
        "with open(os.path.join(save_path, 'movie_mapping.json'), 'w') as f:\n",
        "    json.dump({\n",
        "        'movie_to_idx': movie_mapper.movie_to_idx,\n",
        "        'movie_names': movie_mapper.movie_names\n",
        "    }, f, indent=2, cls=NumpyEncoder)\n",
        "\n",
        "# Salvar histórico de treinamento\n",
        "with open(os.path.join(save_path, 'training_history.json'), 'w') as f:\n",
        "    json.dump({\n",
        "        'baseline': baseline_history,\n",
        "        'rnn': rnn_history,\n",
        "        'multitask': multitask_history,\n",
        "        'full': full_history\n",
        "    }, f, indent=2, cls=NumpyEncoder)\n",
        "\n",
        "print(f\"Modelo salvo em: {save_path}\")\n",
        "print(\"Arquivos salvos:\")\n",
        "for f in os.listdir(save_path):\n",
        "    print(f\"  - {f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSg5HuLzJVL-"
      },
      "outputs": [],
      "source": [
        "# imprime relatorio com os valores encontrados\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RELATÓRIO FINAL\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOG0Lo6JrzP5"
      },
      "source": [
        "## 16. Conclusão\n",
        "\n",
        "Esta implementação adapta o artigo \"BERT one-shot movie recommender system\" de Trung Nguyen (Stanford CS224N) para usar SBERT (Sentence-BERT).\n",
        "\n",
        "### Resultados Principais:\n",
        "\n",
        "| Configuração | nDCG@10 (Artigo BERT) | nDCG@10 (SBERT) |\n",
        "|-------------|------------------|----------------------|\n",
        "| Baseline | 0.130 | Veja resultados acima |\n",
        "| + RNN | 0.165 | Veja resultados acima |\n",
        "| + Multi-Task | 0.138 | Veja resultados acima |\n",
        "| + RNN + Multi-Task | 0.169 | Veja resultados acima |\n",
        "\n",
        "### Insights:\n",
        "1. O RNN para features colaborativas melhora significativamente os resultados\n",
        "2. Multi-task learning com tags oferece ganho marginal\n",
        "3. A combinação de ambas técnicas produz o melhor resultado\n",
        "4. SBERT usa mean pooling em vez de apenas [CLS], potencialmente capturando melhor o contexto\n",
        "5. O dataset pequeno e a natureza concatenada dos dados limitam o desempenho\n",
        "\n",
        "### Referências:\n",
        "- Nguyen, T. (2024). BERT one-shot movie recommender system. Stanford CS224N.\n",
        "- Reimers & Gurevych (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. EMNLP.\n",
        "- Li et al. (2018). Towards deep conversational recommendations. NeurIPS.\n",
        "- Penha & Hauff (2020). What does BERT know about books, movies and music? RecSys."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzP5gJftrzP5"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"IMPLEMENTAÇÃO SBERT COMPLETA!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nPara continuar o treinamento com mais épocas, ajuste:\")\n",
        "print(\"  config.num_epochs = 200  # Conforme artigo original\")\n",
        "print(\"\\nPara usar o modelo treinado:\")\n",
        "print(\"  inference = MovieRecommenderInference(full_model, sbert_tokenizer, movie_mapper, device)\")\n",
        "print(\"  recs = inference.recommend('I like action movies')\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "x9ml9dNSrzPz",
        "HDrse0LLrzP0",
        "qI3m0bgxTjSy",
        "0mvwZO-SEGIz",
        "Brpc9myKrzP1"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UFG-PPGCC-NLP-Final-Project/movie-recommender/blob/main/colab/bert_movie_recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emp9JbsOrzPx"
      },
      "source": [
        "# BERT One-Shot Movie Recommender System\n",
        "\n",
        "**Implementação baseada no artigo**: *BERT one-shot movie recommender system* - Trung Nguyen, Stanford CS224N\n",
        "\n",
        "Este notebook implementa um sistema de recomendação de filmes end-to-end usando BERT, projetado para produzir recomendações estruturadas a partir de queries não estruturadas.\n",
        "\n",
        "## Arquitetura\n",
        "- **Baseline**: BERT + FFN para classificação multi-label\n",
        "- **Extensão 1**: BERT + RNN para features colaborativas\n",
        "- **Extensão 2**: Multi-task learning com dados de tags de usuários\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9ml9dNSrzPz"
      },
      "source": [
        "## 1. Configuração do Ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MRd55n-xrzPz"
      },
      "outputs": [],
      "source": [
        "# Verificar GPU disponível\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ORJobH3LrzP0"
      },
      "outputs": [],
      "source": [
        "# Instalar dependências\n",
        "!pip install -q transformers datasets torch accelerate scikit-learn pandas numpy tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JF2beBfgrzP0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import ndcg_score\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "# Configurar seeds para reprodutibilidade\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Configurar device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Usando device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memória total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Bf4714d3DkuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDrse0LLrzP0"
      },
      "source": [
        "## 2. Configurações e Hiperparâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8K21S8x6rzP0"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    \"\"\"Configurações do modelo e treinamento baseadas no artigo\"\"\"\n",
        "\n",
        "    # Modelo\n",
        "    bert_model_name = 'bert-base-uncased'\n",
        "    bert_hidden_size = 768\n",
        "\n",
        "    # RNN para features colaborativas\n",
        "    rnn_embedding_size = 256\n",
        "    rnn_hidden_size = 128\n",
        "\n",
        "    # FFN\n",
        "    ffn_hidden_size = 256\n",
        "    dropout_prob = 0.3\n",
        "\n",
        "    # Treinamento\n",
        "    movies_batch_size = 8  # Conforme artigo\n",
        "    tags_batch_size = 64   # Conforme artigo\n",
        "    learning_rate = 1e-5   # Conforme artigo\n",
        "\n",
        "    num_epochs = 30        # Reduzido para demonstração (artigo usa 200)\n",
        "\n",
        "    warmup_ratio = 0.1\n",
        "    max_seq_length = 512\n",
        "\n",
        "    # Dataset\n",
        "    num_movies = 6924      # Conforme artigo\n",
        "\n",
        "    # Avaliação\n",
        "    eval_k = 10            # nDCG@10\n",
        "\n",
        "    # Checkpoints\n",
        "    save_dir = './checkpoints'\n",
        "\n",
        "config = Config()\n",
        "os.makedirs(config.save_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Pu6zliCrzP1"
      },
      "source": [
        "## 3. Carregamento e Processamento dos Dados\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI3m0bgxTjSy"
      },
      "source": [
        "### 3.1 Dataset ReDial\n",
        "O dataset ReDial contém diálogos de recomendação de filmes entre um iniciador e um respondente."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1.1 Acesso ao dataset"
      ],
      "metadata": {
        "id": "0mvwZO-SEGIz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5gmYyA8rzP1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Carregar dataset ReDial\n",
        "print(\"Carregando dataset ReDial...\")\n",
        "redial_dataset_raw = load_dataset('community-datasets/re_dial')\n",
        "print(f\"Train: {len(redial_dataset_raw['train'])} exemplos\")\n",
        "print(f\"Test: {len(redial_dataset_raw['test'])} exemplos\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Visualizar estrutura de um exemplo\n",
        "sample = redial_dataset_raw['train'][0]\n",
        "print(\"Estrutura de um exemplo:\")\n",
        "for key in sample.keys():\n",
        "    print(f\"  {key}: {type(sample[key])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V3QeabSrzP1"
      },
      "outputs": [],
      "source": [
        "class MovieIDMapper:\n",
        "    \"\"\"\n",
        "    Mapeia IDs de filmes entre diferentes formatos a partir do dataset ReDial. Exemplo: '@123' -> 123\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.movie_to_idx = {}\n",
        "        self.idx_to_movie = {}\n",
        "        self.movie_names = {}\n",
        "\n",
        "    def build_from_dataset(self, dataset):\n",
        "        \"\"\"Constrói mapeamento a partir do dataset ReDial\"\"\"\n",
        "        all_movies = set()\n",
        "\n",
        "        for split in ['train', 'test']:\n",
        "            for original_content in dataset[split]:\n",
        "                # Extrair IDs de filmes das mensagens\n",
        "                messages = original_content.get('messages', [])\n",
        "                for msg in messages:\n",
        "                    text = msg.get('text', '')\n",
        "                    movie_ids = re.findall(r'@(\\d+)', text)\n",
        "                    all_movies.update(movie_ids)\n",
        "\n",
        "                # Extrair dos movieMentions\n",
        "                mentions = original_content.get('movieMentions', {})\n",
        "                if isinstance(mentions, dict):\n",
        "                    for movie_id, name in mentions.items():\n",
        "                        all_movies.add(str(movie_id).replace('@', ''))\n",
        "                        self.movie_names[str(movie_id).replace('@', '')] = name\n",
        "\n",
        "        # Criar mapeamento ordenado\n",
        "        sorted_movies = sorted([int(m) for m in all_movies if m.isdigit()])\n",
        "\n",
        "        for idx, movie_id in enumerate(sorted_movies):\n",
        "            self.movie_to_idx[str(movie_id)] = idx\n",
        "            self.idx_to_movie[idx] = str(movie_id)\n",
        "\n",
        "        print(f\"Total de filmes únicos: {len(self.movie_to_idx)}\")\n",
        "        return self\n",
        "\n",
        "    def get_num_movies(self):\n",
        "        return len(self.movie_to_idx)\n",
        "\n",
        "    def movie_id_to_idx(self, movie_id):\n",
        "        movie_id = str(movie_id).replace('@', '')\n",
        "        return self.movie_to_idx.get(movie_id, -1)\n",
        "\n",
        "    def idx_to_movie_id(self, idx):\n",
        "        return self.idx_to_movie.get(idx, None)\n",
        "\n",
        "# Construir mapeamento\n",
        "movie_mapper = MovieIDMapper().build_from_dataset(redial_dataset_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brpc9myKrzP1"
      },
      "source": [
        "#### 3.1.2 Processamento dos Diálogos\n",
        "\n",
        "Conforme o artigo, concatenamos as utterances do iniciador com tokens [SEP] e usamos as recomendações do respondente como labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yTMSjZArzP1"
      },
      "outputs": [],
      "source": [
        "def process_dialogue(example, movie_mapper):\n",
        "    \"\"\"\n",
        "    Processa um diálogo do ReDial conforme descrito no artigo:\n",
        "    - Input: utterances do iniciador concatenadas com [SEP]\n",
        "    - Output: IDs dos filmes recomendados pelo respondente\n",
        "    - Movies mentioned: filmes mencionados pelo iniciador (para RNN)\n",
        "    \"\"\"\n",
        "    messages = example.get('messages', [])\n",
        "\n",
        "    initiator_texts = []\n",
        "    mentioned_movies = []  # Filmes mencionados pelo iniciador\n",
        "    recommended_movies = []  # Filmes recomendados pelo respondente\n",
        "\n",
        "    for msg in messages:\n",
        "        text = msg.get('text', '')\n",
        "        sender_id = msg.get('senderWorkerId', 0)\n",
        "\n",
        "        # Extrair IDs de filmes\n",
        "        movie_ids = re.findall(r'@(\\d+)', text)\n",
        "\n",
        "        # Determinar se é iniciador (primeiro sender) ou respondente\n",
        "        if sender_id == messages[0].get('senderWorkerId', 0):\n",
        "            # Iniciador - adicionar texto e filmes mencionados\n",
        "            # Substituir IDs por placeholder para versão sem RNN\n",
        "            clean_text = re.sub(r'@\\d+', '@', text)\n",
        "            initiator_texts.append(clean_text)\n",
        "            mentioned_movies.extend(movie_ids)\n",
        "        else:\n",
        "            # Respondente - coletar recomendações\n",
        "            recommended_movies.extend(movie_ids)\n",
        "\n",
        "    # Concatenar textos do iniciador com [SEP]\n",
        "    input_text = ' [SEP] '.join(initiator_texts)\n",
        "\n",
        "    # Converter IDs para índices\n",
        "    mentioned_indices = [movie_mapper.movie_id_to_idx(m) for m in mentioned_movies]\n",
        "    mentioned_indices = [idx for idx in mentioned_indices if idx >= 0]\n",
        "\n",
        "    recommended_indices = [movie_mapper.movie_id_to_idx(m) for m in recommended_movies]\n",
        "    recommended_indices = list(set([idx for idx in recommended_indices if idx >= 0]))\n",
        "\n",
        "    return {\n",
        "        'input_text': input_text,\n",
        "        'input_text_with_ids': ' [SEP] '.join([msg.get('text', '') for msg in messages\n",
        "                                               if msg.get('senderWorkerId') == messages[0].get('senderWorkerId')]),\n",
        "        'mentioned_movies': mentioned_indices,\n",
        "        'recommended_movies': recommended_indices\n",
        "    }\n",
        "\n",
        "# Processar dataset\n",
        "def process_split(dataset_split, movie_mapper):\n",
        "    processed = []\n",
        "    for example in tqdm(dataset_split, desc=\"Processando\"):\n",
        "        proc = process_dialogue(example, movie_mapper)\n",
        "        # Filtrar exemplos sem recomendações\n",
        "        if proc['recommended_movies'] and proc['input_text'].strip():\n",
        "            processed.append(proc)\n",
        "    return processed\n",
        "\n",
        "print(\"Processando split de treino...\")\n",
        "train_data = process_split(redial_dataset_raw['train'], movie_mapper)\n",
        "print(f\"Exemplos de treino válidos: {len(train_data)}\")\n",
        "\n",
        "print(\"\\nProcessando split de teste...\")\n",
        "test_data = process_split(redial_dataset_raw['test'], movie_mapper)\n",
        "print(f\"Exemplos de teste válidos: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jdiP4qAWrzP2"
      },
      "outputs": [],
      "source": [
        "# Visualizar exemplo processado\n",
        "print(\"Exemplo processado:\")\n",
        "print(f\"Input text: {train_data[0]['input_text'][:500]}...\")\n",
        "print(f\"\\nMentioned movies (índices): {train_data[0]['mentioned_movies'][:5]}\")\n",
        "print(f\"Recommended movies (índices): {train_data[0]['recommended_movies']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EOQYxyVrzP2"
      },
      "source": [
        "### 3.2 Dataset MovieLens\n",
        "\n",
        "Para o experimento de multi-task learning, usamos tags de usuários do MovieLens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ-Adm7vrzP2"
      },
      "outputs": [],
      "source": [
        "# Download MovieLens tags\n",
        "!wget -q -nc http://files.grouplens.org/datasets/movielens/ml-latest.zip\n",
        "!unzip -q -o ml-latest.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PLyP7aa0rzP2"
      },
      "outputs": [],
      "source": [
        "# Carregar dados do MovieLens\n",
        "tags_csv = pd.read_csv('ml-latest/tags.csv')\n",
        "movies_csv = pd.read_csv('ml-latest/movies.csv')\n",
        "\n",
        "print(f\"Tags totais: {len(tags_csv)}\")\n",
        "print(f\"Filmes totais: {len(movies_csv)}\")\n",
        "print(f\"\\nExemplo de tags:\")\n",
        "print(tags_csv.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zbQ8_hmHrzP2"
      },
      "outputs": [],
      "source": [
        "def create_tag_dataset(tags_csv, movie_mapper, max_tags_per_movie=50):\n",
        "    \"\"\"\n",
        "    Cria dataset de tags para multi-task learning\n",
        "    Input: tag text\n",
        "    Output: movie index\n",
        "    \"\"\"\n",
        "    tag_data = []\n",
        "\n",
        "    # Agrupar tags por filme\n",
        "    for movie_id, group in tags_csv.groupby('movieId'):\n",
        "        movie_idx = movie_mapper.movie_id_to_idx(str(movie_id))\n",
        "        if movie_idx < 0:\n",
        "            continue\n",
        "\n",
        "        tags = group['tag'].tolist()[:max_tags_per_movie]\n",
        "        for tag in tags:\n",
        "            if isinstance(tag, str) and len(tag.strip()) > 2:\n",
        "                tag_data.append({\n",
        "                    'tag_text': tag.strip(),\n",
        "                    'movie_idx': movie_idx\n",
        "                })\n",
        "\n",
        "    return tag_data\n",
        "\n",
        "# Criar dataset de tags (movie_mapper definido junto do dataset do redial)\n",
        "tag_data = create_tag_dataset(tags_csv, movie_mapper)\n",
        "print(f\"Exemplos de tags: {len(tag_data)}\")\n",
        "print(f\"\\nExemplo de tag:\")\n",
        "print(tag_data[0])\n",
        "print(\"/n\")\n",
        "\n",
        "# Split treino/teste para tags\n",
        "random.shuffle(tag_data)\n",
        "split_idx = int(len(tag_data) * 0.9)\n",
        "tag_train_data = tag_data[:split_idx]\n",
        "tag_test_data = tag_data[split_idx:]\n",
        "\n",
        "print(f\"Tags treino: {len(tag_train_data)}\")\n",
        "print(f\"Tags teste: {len(tag_test_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-LaEGBNrzP2"
      },
      "source": [
        "## 4. Dataset Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JNQKLu1rzP2"
      },
      "outputs": [],
      "source": [
        "class MovieRecommendationDataset(Dataset):\n",
        "    \"\"\"Dataset para recomendação de filmes (tarefa principal)\"\"\"\n",
        "\n",
        "    def __init__(self, data, tokenizer, num_movies, max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.num_movies = num_movies\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        # Tokenizar input\n",
        "        encoding = self.tokenizer(\n",
        "            item['input_text'],\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Criar label multi-hot\n",
        "        labels = torch.zeros(self.num_movies)\n",
        "        for movie_idx in item['recommended_movies']:\n",
        "            if 0 <= movie_idx < self.num_movies:\n",
        "                labels[movie_idx] = 1.0\n",
        "\n",
        "        # Filmes mencionados (para RNN)\n",
        "        mentioned = item['mentioned_movies'][:20]  # Limitar\n",
        "        mentioned_tensor = torch.zeros(20, dtype=torch.long)\n",
        "        mentioned_mask = torch.zeros(20, dtype=torch.bool)\n",
        "\n",
        "        for i, m_idx in enumerate(mentioned):\n",
        "            if i < 20 and 0 <= m_idx < self.num_movies:\n",
        "                mentioned_tensor[i] = m_idx\n",
        "                mentioned_mask[i] = True\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'labels': labels,\n",
        "            'mentioned_movies': mentioned_tensor,\n",
        "            'mentioned_mask': mentioned_mask\n",
        "        }\n",
        "\n",
        "\n",
        "class TagDataset(Dataset):\n",
        "    \"\"\"Dataset para predição de filme a partir de tag (tarefa auxiliar)\"\"\"\n",
        "\n",
        "    def __init__(self, data, tokenizer, max_length=64):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            item['tag_text'],\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(item['movie_idx'], dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYnXsEbcrzP2"
      },
      "outputs": [],
      "source": [
        "# Inicializar tokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(config.bert_model_name)\n",
        "print(f\"Cria o tokenizador do Modelo: {config.bert_model_name}\")\n",
        "\n",
        "# Atualizar número de filmes baseado no mapeamento real\n",
        "config.num_movies = movie_mapper.get_num_movies()\n",
        "print(f\"Número de filmes: {config.num_movies}\")\n",
        "\n",
        "# Criar datasets\n",
        "train_dataset = MovieRecommendationDataset(\n",
        "    train_data, bert_tokenizer, config.num_movies, config.max_seq_length\n",
        ")\n",
        "test_dataset = MovieRecommendationDataset(\n",
        "    test_data, bert_tokenizer, config.num_movies, config.max_seq_length\n",
        ")\n",
        "\n",
        "tag_train_dataset = TagDataset(tag_train_data, bert_tokenizer)\n",
        "tag_test_dataset = TagDataset(tag_test_data, bert_tokenizer)\n",
        "\n",
        "print(f\"\\nDataset de treino: {len(train_dataset)} exemplos\")\n",
        "print(f\"Dataset de teste: {len(test_dataset)} exemplos\")\n",
        "print(f\"Dataset de tags treino: {len(tag_train_dataset)} exemplos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwkJ0yNgrzP2"
      },
      "source": [
        "## 5. Arquitetura dos Modelos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri8m01PDTVpi"
      },
      "source": [
        "### 5.1 Modelo Baseline: BERT + FFN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V0YwgyIrzP2"
      },
      "outputs": [],
      "source": [
        "class BERTMovieRecommender(nn.Module):\n",
        "    \"\"\"\n",
        "    Modelo baseline: BERT + FFN para classificação multi-label.\n",
        "\n",
        "    f(U) = FFN(BERT_CLS(U))\n",
        "\n",
        "    onde U é o input concatenado com [SEP] tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained(config.bert_model_name)\n",
        "\n",
        "        # FFN para projeção\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(config.dropout_prob),\n",
        "            nn.Linear(config.bert_hidden_size, config.ffn_hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.dropout_prob),\n",
        "            nn.Linear(config.ffn_hidden_size, config.num_movies)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, **kwargs):\n",
        "        # Encoding BERT\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        # Usar [CLS] token\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Projeção para logits\n",
        "        logits = self.classifier(cls_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def get_cls_embedding(self, input_ids, attention_mask):\n",
        "        \"\"\"Retorna embedding do [CLS] para multi-task\"\"\"\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        return outputs.last_hidden_state[:, 0, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcbblvYnrzP2"
      },
      "source": [
        "### 5.2 Modelo com RNN para Features Colaborativas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OucrIy1irzP2"
      },
      "outputs": [],
      "source": [
        "class BERTRNNMovieRecommender(nn.Module):\n",
        "    \"\"\"\n",
        "    Modelo com RNN para aprender features colaborativas.\n",
        "\n",
        "    f(U) = FFN(BERT_CLS(U), RNN(L(U)))\n",
        "\n",
        "    onde L(U) é a lista de filmes mencionados em U.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained(config.bert_model_name)\n",
        "\n",
        "        # Embedding de filmes para RNN\n",
        "        self.movie_embedding = nn.Embedding(\n",
        "            config.num_movies + 1,  # +1 para padding\n",
        "            config.rnn_embedding_size,\n",
        "            padding_idx=config.num_movies\n",
        "        )\n",
        "\n",
        "        # RNN para processar sequência de filmes mencionados\n",
        "        self.rnn = nn.GRU(\n",
        "            input_size=config.rnn_embedding_size,\n",
        "            hidden_size=config.rnn_hidden_size,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Dimensão combinada: BERT CLS + RNN output (bidirectional)\n",
        "        combined_size = config.bert_hidden_size + (config.rnn_hidden_size * 2)\n",
        "\n",
        "        # FFN para projeção\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(config.dropout_prob),\n",
        "            nn.Linear(combined_size, config.ffn_hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.dropout_prob),\n",
        "            nn.Linear(config.ffn_hidden_size, config.num_movies)\n",
        "        )\n",
        "\n",
        "        self.num_movies = config.num_movies\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, mentioned_movies, mentioned_mask, **kwargs):\n",
        "        batch_size = input_ids.size(0)\n",
        "\n",
        "        # Encoding BERT\n",
        "        bert_outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        cls_output = bert_outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Processar filmes mencionados com RNN\n",
        "        # Substituir índices inválidos pelo índice de padding\n",
        "        mentioned_movies = mentioned_movies.clone()\n",
        "        mentioned_movies[~mentioned_mask] = self.num_movies\n",
        "\n",
        "        movie_embeds = self.movie_embedding(mentioned_movies)\n",
        "\n",
        "        # RNN\n",
        "        rnn_output, hidden = self.rnn(movie_embeds)\n",
        "\n",
        "        # Usar último hidden state (concatenado de ambas direções)\n",
        "        rnn_features = hidden.transpose(0, 1).contiguous().view(batch_size, -1)\n",
        "\n",
        "        # Combinar features\n",
        "        combined = torch.cat([cls_output, rnn_features], dim=-1)\n",
        "\n",
        "        # Projeção para logits\n",
        "        logits = self.classifier(combined)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def get_cls_embedding(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        return outputs.last_hidden_state[:, 0, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKyimV0KrzP3"
      },
      "source": [
        "### 5.3 Wrapper para Multi-Task Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mn7QJfChrzP3"
      },
      "outputs": [],
      "source": [
        "class MultiTaskWrapper(nn.Module):\n",
        "    \"\"\"\n",
        "    Wrapper para multi-task learning com tarefa auxiliar de tags.\n",
        "\n",
        "    Loss = BCE(f(U), y) + CE(f(U), z)\n",
        "\n",
        "    onde z é o filme correto para uma tag.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_model, config):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "\n",
        "        # Head separado para tarefa de tags (usa mesmo pathway do BERT)\n",
        "        self.tag_classifier = nn.Sequential(\n",
        "            nn.Dropout(config.dropout_prob),\n",
        "            nn.Linear(config.bert_hidden_size, config.ffn_hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.dropout_prob),\n",
        "            nn.Linear(config.ffn_hidden_size, config.num_movies)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, **kwargs):\n",
        "        return self.base_model(input_ids, attention_mask, **kwargs)\n",
        "\n",
        "    def forward_tags(self, input_ids, attention_mask):\n",
        "        \"\"\"Forward para tarefa de tags\"\"\"\n",
        "        cls_embedding = self.base_model.get_cls_embedding(input_ids, attention_mask)\n",
        "        return self.tag_classifier(cls_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0egapTHcrzP3"
      },
      "source": [
        "## 6. Métricas de Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIqgo5V4rzP3"
      },
      "outputs": [],
      "source": [
        "def compute_ndcg_at_k(predictions, labels, k=10):\n",
        "    \"\"\"\n",
        "    Calcula nDCG@k conforme usado no artigo.\n",
        "\n",
        "    Args:\n",
        "        predictions: tensor de logits (batch_size, num_movies)\n",
        "        labels: tensor multi-hot de labels (batch_size, num_movies)\n",
        "        k: número de itens para considerar\n",
        "\n",
        "    Returns:\n",
        "        nDCG@k médio\n",
        "    \"\"\"\n",
        "    predictions = predictions.detach().cpu().numpy()\n",
        "    labels = labels.detach().cpu().numpy()\n",
        "\n",
        "    ndcg_scores = []\n",
        "\n",
        "    for pred, label in zip(predictions, labels):\n",
        "        # Se não há labels positivos, pular\n",
        "        if label.sum() == 0:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            score = ndcg_score([label], [pred], k=k)\n",
        "            ndcg_scores.append(score)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    return np.mean(ndcg_scores) if ndcg_scores else 0.0\n",
        "\n",
        "\n",
        "def compute_recall_at_k(predictions, labels, k=10):\n",
        "    \"\"\"Calcula Recall@k\"\"\"\n",
        "    predictions = predictions.detach().cpu().numpy()\n",
        "    labels = labels.detach().cpu().numpy()\n",
        "\n",
        "    recalls = []\n",
        "\n",
        "    for pred, label in zip(predictions, labels):\n",
        "        if label.sum() == 0:\n",
        "            continue\n",
        "\n",
        "        top_k_indices = np.argsort(pred)[-k:]\n",
        "        relevant = label[top_k_indices].sum()\n",
        "        total_relevant = label.sum()\n",
        "\n",
        "        recalls.append(relevant / total_relevant)\n",
        "\n",
        "    return np.mean(recalls) if recalls else 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuC_leOxrzP3"
      },
      "source": [
        "## 7. Loop de Treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKqrpZZPrzP3"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    \"\"\"\n",
        "    Trainer para os modelos de recomendação\n",
        "\n",
        "    Características:\n",
        "    - Checkpoints por experimento (pasta única)\n",
        "    - Salva melhor modelo (best_model.pt)\n",
        "    - Salva última época (last_checkpoint.pt)\n",
        "    - Permite retomar treinamento\n",
        "    - Logging em arquivo\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, config, train_loader, eval_loader,\n",
        "                 tag_train_loader=None, tag_eval_loader=None,\n",
        "                 use_multitask=False, experiment_name=\"exp\"):\n",
        "\n",
        "        self.model = model.to(device)\n",
        "        self.config = config\n",
        "        self.train_loader = train_loader\n",
        "        self.eval_loader = eval_loader\n",
        "        self.tag_train_loader = tag_train_loader\n",
        "        self.tag_eval_loader = tag_eval_loader\n",
        "        self.use_multitask = use_multitask\n",
        "        self.experiment_name = experiment_name\n",
        "\n",
        "        # ✅ Criar diretório específico do experimento\n",
        "        self.checkpoint_dir = os.path.join(config.save_dir, experiment_name)\n",
        "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Experimento: {experiment_name}\")\n",
        "        print(f\"Checkpoint directory: {self.checkpoint_dir}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = AdamW(\n",
        "            model.parameters(),\n",
        "            lr=config.learning_rate\n",
        "        )\n",
        "\n",
        "        # Scheduler\n",
        "        total_steps = len(train_loader) * config.num_epochs\n",
        "        warmup_steps = int(total_steps * config.warmup_ratio)\n",
        "\n",
        "        self.scheduler = get_linear_schedule_with_warmup(\n",
        "            self.optimizer,\n",
        "            num_warmup_steps=warmup_steps,\n",
        "            num_training_steps=total_steps\n",
        "        )\n",
        "\n",
        "        # ✅ Calcular pos_weight para balancear classes\n",
        "        print(\"\\nCalculando pos_weight para balancear classes...\")\n",
        "\n",
        "        sample_labels = []\n",
        "        num_batches_to_sample = min(100, len(train_loader))\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            sample_labels.append(batch['labels'])\n",
        "            if i >= num_batches_to_sample - 1:\n",
        "                break\n",
        "\n",
        "        sample_labels = torch.cat(sample_labels, dim=0)\n",
        "\n",
        "        num_positives = sample_labels.sum()\n",
        "        num_negatives = sample_labels.numel() - num_positives\n",
        "        pos_weight_value = (num_negatives / num_positives).item()\n",
        "\n",
        "        print(f\"  • Amostras analisadas: {len(sample_labels):,}\")\n",
        "        print(f\"  • Labels positivos: {num_positives.item():,}\")\n",
        "        print(f\"  • Labels negativos: {num_negatives.item():,}\")\n",
        "        print(f\"  • Taxa de positivos: {num_positives/sample_labels.numel()*100:.4f}%\")\n",
        "        print(f\"  • pos_weight calculado: {pos_weight_value:.1f}\")\n",
        "\n",
        "        # Loss functions\n",
        "        self.bce_loss = nn.BCEWithLogitsLoss(\n",
        "            pos_weight=torch.full((config.num_movies,), pos_weight_value).to(device)\n",
        "        )\n",
        "\n",
        "        self.ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Histórico\n",
        "        self.history = {\n",
        "            'train_loss': [],\n",
        "            'eval_loss': [],\n",
        "            'ndcg': [],\n",
        "            'recall': []\n",
        "        }\n",
        "\n",
        "        # ✅ Arquivo de log\n",
        "        self.log_file = os.path.join(self.checkpoint_dir, 'training_log.txt')\n",
        "\n",
        "        # Inicializar log\n",
        "        with open(self.log_file, 'w') as f:\n",
        "            f.write(f\"{'='*80}\\n\")\n",
        "            f.write(f\"Training Log - {experiment_name}\\n\")\n",
        "            f.write(f\"{'='*80}\\n\")\n",
        "            f.write(f\"Timestamp: {datetime.now()}\\n\")\n",
        "            f.write(f\"Model: {model.__class__.__name__}\\n\")\n",
        "            f.write(f\"Learning rate: {config.learning_rate}\\n\")\n",
        "            f.write(f\"Batch size: {config.movies_batch_size}\\n\")\n",
        "            f.write(f\"Epochs: {config.num_epochs}\\n\")\n",
        "            f.write(f\"pos_weight: {pos_weight_value:.1f}\\n\")\n",
        "            f.write(f\"{'='*80}\\n\\n\")\n",
        "\n",
        "        print(f\"  • Log file: {self.log_file}\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "    def train_epoch(self):\n",
        "        \"\"\"Treina uma época\"\"\"\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        tag_iter = iter(self.tag_train_loader) if self.use_multitask and self.tag_train_loader else None\n",
        "\n",
        "        progress_bar = tqdm(self.train_loader, desc=\"Training\")\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass principal\n",
        "            logits = self.model(\n",
        "                input_ids=batch['input_ids'],\n",
        "                attention_mask=batch['attention_mask'],\n",
        "                mentioned_movies=batch.get('mentioned_movies'),\n",
        "                mentioned_mask=batch.get('mentioned_mask')\n",
        "            )\n",
        "\n",
        "            # Loss principal\n",
        "            loss = self.bce_loss(logits, batch['labels'])\n",
        "\n",
        "            # Multi-task: adicionar loss de tags\n",
        "            if self.use_multitask and tag_iter:\n",
        "                try:\n",
        "                    tag_batch = next(tag_iter)\n",
        "                except StopIteration:\n",
        "                    tag_iter = iter(self.tag_train_loader)\n",
        "                    tag_batch = next(tag_iter)\n",
        "\n",
        "                tag_batch = {k: v.to(device) for k, v in tag_batch.items()}\n",
        "\n",
        "                tag_logits = self.model.forward_tags(\n",
        "                    input_ids=tag_batch['input_ids'],\n",
        "                    attention_mask=tag_batch['attention_mask']\n",
        "                )\n",
        "\n",
        "                tag_loss = self.ce_loss(tag_logits, tag_batch['label'])\n",
        "                loss = loss + tag_loss\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "\n",
        "            self.optimizer.step()\n",
        "            self.scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        return total_loss / num_batches\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self):\n",
        "        \"\"\"Avalia o modelo\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "\n",
        "        for batch in tqdm(self.eval_loader, desc=\"Evaluating\"):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            logits = self.model(\n",
        "                input_ids=batch['input_ids'],\n",
        "                attention_mask=batch['attention_mask'],\n",
        "                mentioned_movies=batch.get('mentioned_movies'),\n",
        "                mentioned_mask=batch.get('mentioned_mask')\n",
        "            )\n",
        "\n",
        "            loss = self.bce_loss(logits, batch['labels'])\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            all_predictions.append(logits)\n",
        "            all_labels.append(batch['labels'])\n",
        "\n",
        "        # Concatenar todas as predições\n",
        "        all_predictions = torch.cat(all_predictions, dim=0)\n",
        "        all_labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "        # Calcular métricas\n",
        "        ndcg = compute_ndcg_at_k(all_predictions, all_labels, k=self.config.eval_k)\n",
        "        recall = compute_recall_at_k(all_predictions, all_labels, k=self.config.eval_k)\n",
        "\n",
        "        return {\n",
        "            'loss': total_loss / len(self.eval_loader),\n",
        "            'ndcg@10': ndcg,\n",
        "            'recall@10': recall\n",
        "        }\n",
        "\n",
        "    def save_checkpoint(self, epoch, best_ndcg, is_best=False):\n",
        "        \"\"\"\n",
        "        Salva checkpoint\n",
        "\n",
        "        Args:\n",
        "            epoch: número da época atual\n",
        "            best_ndcg: melhor nDCG até agora\n",
        "            is_best: se é o melhor modelo até agora\n",
        "        \"\"\"\n",
        "        # ✅ Sempre salvar último checkpoint (permite retomar)\n",
        "        last_checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "            'best_ndcg': best_ndcg,\n",
        "            'history': self.history,\n",
        "            'experiment_name': self.experiment_name,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        last_checkpoint_path = os.path.join(self.checkpoint_dir, 'last_checkpoint.pt')\n",
        "        torch.save(last_checkpoint, last_checkpoint_path)\n",
        "\n",
        "        # ✅ Se é o melhor, salvar separadamente\n",
        "        if is_best:\n",
        "            best_model_path = os.path.join(self.checkpoint_dir, 'best_model.pt')\n",
        "            torch.save(self.model.state_dict(), best_model_path)\n",
        "\n",
        "            # Também salvar checkpoint completo do melhor\n",
        "            best_checkpoint_path = os.path.join(self.checkpoint_dir, 'best_checkpoint.pt')\n",
        "            torch.save(last_checkpoint, best_checkpoint_path)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        \"\"\"\n",
        "        Carrega checkpoint se existir\n",
        "\n",
        "        Returns:\n",
        "            (start_epoch, best_ndcg) ou (0, 0) se não existe checkpoint\n",
        "        \"\"\"\n",
        "        last_checkpoint_path = os.path.join(self.checkpoint_dir, 'last_checkpoint.pt')\n",
        "\n",
        "        if not os.path.exists(last_checkpoint_path):\n",
        "            return 0, 0\n",
        "\n",
        "        try:\n",
        "            print(f\"\\nTentando carregar checkpoint de {last_checkpoint_path}...\")\n",
        "            checkpoint = torch.load(last_checkpoint_path, weights_only=False)\n",
        "\n",
        "            # ✅ Verificar compatibilidade antes de carregar\n",
        "            try:\n",
        "                self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "                self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "                start_epoch = checkpoint['epoch']\n",
        "                best_ndcg = checkpoint['best_ndcg']\n",
        "                self.history = checkpoint['history']\n",
        "\n",
        "                print(f\"✅ Checkpoint carregado com sucesso!\")\n",
        "                print(f\"   Retomando da época {start_epoch}\")\n",
        "                print(f\"   Melhor nDCG até agora: {best_ndcg:.4f}\")\n",
        "\n",
        "                return start_epoch, best_ndcg\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                print(f\"⚠️  Checkpoint incompatível: {e}\")\n",
        "                print(f\"⚠️  Ignorando checkpoint e começando do zero\")\n",
        "                return 0, 0\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erro ao carregar checkpoint: {e}\")\n",
        "            print(f\"Começando do zero\")\n",
        "            return 0, 0\n",
        "\n",
        "    def log_epoch(self, epoch, train_loss, eval_metrics):\n",
        "        \"\"\"Salva informações da época no arquivo de log\"\"\"\n",
        "        with open(self.log_file, 'a') as f:\n",
        "            f.write(f\"Epoch {epoch}\\n\")\n",
        "            f.write(f\"  Train Loss: {train_loss:.6f}\\n\")\n",
        "            f.write(f\"  Eval Loss: {eval_metrics['loss']:.6f}\\n\")\n",
        "            f.write(f\"  nDCG@10: {eval_metrics['ndcg@10']:.6f}\\n\")\n",
        "            f.write(f\"  Recall@10: {eval_metrics['recall@10']:.6f}\\n\")\n",
        "            f.write(f\"  Timestamp: {datetime.now()}\\n\")\n",
        "            f.write(\"-\" * 60 + \"\\n\")\n",
        "\n",
        "    def train(self, num_epochs=None, resume=True):\n",
        "        \"\"\"\n",
        "        Treina o modelo\n",
        "\n",
        "        Args:\n",
        "            num_epochs: número de épocas (None usa config.num_epochs)\n",
        "            resume: se deve tentar retomar de checkpoint\n",
        "\n",
        "        Returns:\n",
        "            history: dicionário com histórico de treinamento\n",
        "        \"\"\"\n",
        "\n",
        "        num_epochs = num_epochs or self.config.num_epochs\n",
        "        best_ndcg = 0\n",
        "        start_epoch = 0\n",
        "\n",
        "        # ✅ Tentar retomar de checkpoint\n",
        "        if resume:\n",
        "            start_epoch, best_ndcg = self.load_checkpoint()\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Iniciando treinamento: {self.experiment_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Épocas: {start_epoch + 1} → {num_epochs}\")\n",
        "        print(f\"Checkpoint dir: {self.checkpoint_dir}\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        for epoch in range(start_epoch, num_epochs):\n",
        "            epoch_start_time = datetime.now()\n",
        "\n",
        "            print(f\"\\n{'='*50}\")\n",
        "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "            print('='*50)\n",
        "\n",
        "            # Treinar\n",
        "            train_loss = self.train_epoch()\n",
        "            self.history['train_loss'].append(train_loss)\n",
        "\n",
        "            # Avaliar\n",
        "            eval_metrics = self.evaluate()\n",
        "            self.history['eval_loss'].append(eval_metrics['loss'])\n",
        "            self.history['ndcg'].append(eval_metrics['ndcg@10'])\n",
        "            self.history['recall'].append(eval_metrics['recall@10'])\n",
        "\n",
        "            # Tempo da época\n",
        "            epoch_time = (datetime.now() - epoch_start_time).total_seconds()\n",
        "\n",
        "            # Print resultados\n",
        "            print(f\"\\nTrain Loss: {train_loss:.4f}\")\n",
        "            print(f\"Eval Loss: {eval_metrics['loss']:.4f}\")\n",
        "            print(f\"nDCG@10: {eval_metrics['ndcg@10']:.4f}\")\n",
        "            print(f\"Recall@10: {eval_metrics['recall@10']:.4f}\")\n",
        "            print(f\"Tempo: {epoch_time:.1f}s\")\n",
        "\n",
        "            # ✅ Verificar se é o melhor modelo\n",
        "            is_best = eval_metrics['ndcg@10'] > best_ndcg\n",
        "\n",
        "            if is_best:\n",
        "                best_ndcg = eval_metrics['ndcg@10']\n",
        "                print(f\"🌟 Novo melhor modelo! nDCG@10: {best_ndcg:.4f}\")\n",
        "\n",
        "            # ✅ Salvar checkpoints\n",
        "            self.save_checkpoint(epoch + 1, best_ndcg, is_best)\n",
        "\n",
        "            # ✅ Log em arquivo\n",
        "            self.log_epoch(epoch + 1, train_loss, eval_metrics)\n",
        "\n",
        "            if (epoch + 1) % 5 == 0:\n",
        "                self.backup_to_drive()\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Treinamento concluído: {self.experiment_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Melhor nDCG@10: {best_ndcg:.4f}\")\n",
        "        print(f\"Checkpoints salvos em: {self.checkpoint_dir}\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        # ✅ Salvar relatório final\n",
        "        self.save_final_report(best_ndcg)\n",
        "\n",
        "        return self.history\n",
        "\n",
        "    def save_final_report(self, best_ndcg):\n",
        "        \"\"\"Salva relatório final em JSON\"\"\"\n",
        "        import json\n",
        "        from datetime import datetime\n",
        "\n",
        "        report = {\n",
        "            'experiment': self.experiment_name,\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'model': self.model.__class__.__name__,\n",
        "            'config': {\n",
        "                'learning_rate': self.config.learning_rate,\n",
        "                'num_epochs': self.config.num_epochs,\n",
        "                'batch_size': self.config.movies_batch_size,\n",
        "                'dropout': self.config.dropout_prob,\n",
        "            },\n",
        "            'results': {\n",
        "                'best_ndcg@10': float(best_ndcg),\n",
        "                'best_recall@10': float(max(self.history['recall'])),\n",
        "                'final_train_loss': float(self.history['train_loss'][-1]),\n",
        "                'final_eval_loss': float(self.history['eval_loss'][-1]),\n",
        "                'epochs_trained': len(self.history['ndcg']),\n",
        "            },\n",
        "            'history': {\n",
        "                'train_loss': [float(x) for x in self.history['train_loss']],\n",
        "                'eval_loss': [float(x) for x in self.history['eval_loss']],\n",
        "                'ndcg': [float(x) for x in self.history['ndcg']],\n",
        "                'recall': [float(x) for x in self.history['recall']],\n",
        "            }\n",
        "        }\n",
        "\n",
        "        report_path = os.path.join(self.checkpoint_dir, 'report.json')\n",
        "        with open(report_path, 'w') as f:\n",
        "            json.dump(report, f, indent=2)\n",
        "\n",
        "        print(f\"📊 Relatório salvo em: {report_path}\")\n",
        "\n",
        "\n",
        "    def backup_to_drive(self):\n",
        "        \"\"\"Backup apenas de logs e relatórios (sem modelos)\"\"\"\n",
        "        import shutil\n",
        "        from datetime import datetime\n",
        "\n",
        "        if not os.path.exists('/content/drive'):\n",
        "            drive.mount('/content/drive')\n",
        "\n",
        "        # Criar timestamp único para esta execução (uma vez por trainer)\n",
        "        if not hasattr(self, '_execution_timestamp'):\n",
        "            self._execution_timestamp = datetime.now().strftime('%Y%m%d-%H%M')\n",
        "\n",
        "        # Estrutura: /logs/execucao-YYYYMMDD-HHMM/experimento/\n",
        "        drive_path = f'/content/drive/MyDrive/movie_recommender_logs/execucao-{self._execution_timestamp}/{self.experiment_name}'\n",
        "        os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "        # Copiar apenas arquivos leves\n",
        "        files_to_backup = [\n",
        "            'training_log.txt',\n",
        "            'report.json',\n",
        "            'training_curves.png'\n",
        "        ]\n",
        "\n",
        "        for filename in files_to_backup:\n",
        "            src = os.path.join(self.checkpoint_dir, filename)\n",
        "            if os.path.exists(src):\n",
        "                shutil.copy2(src, os.path.join(drive_path, filename))\n",
        "\n",
        "        print(f\"💾 Logs salvos no Drive: execucao-{self._execution_timestamp}/{self.experiment_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGr2PuvzrzP3"
      },
      "outputs": [],
      "source": [
        "# Criar DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config.movies_batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "eval_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config.movies_batch_size * 2,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"Batches de treino: {len(train_loader)}\")\n",
        "print(f\"Batches de avaliação: {len(eval_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNds_WmjrzP3"
      },
      "source": [
        "## 8. Experimento 1: Baseline BERT + FFN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "794rVjELrzP3"
      },
      "outputs": [],
      "source": [
        "# Treinar modelo baseline\n",
        "print(\"=\"*60)\n",
        "print(\"EXPERIMENTO 1: BERT Baseline (sem RNN, sem multi-task)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "baseline_model = BERTMovieRecommender(config)\n",
        "\n",
        "baseline_trainer = Trainer(\n",
        "    model=baseline_model,\n",
        "    config=config,\n",
        "    train_loader=train_loader,\n",
        "    eval_loader=eval_loader,\n",
        "    use_multitask=False,\n",
        "    experiment_name=\"exp1_baseline\"\n",
        ")\n",
        "\n",
        "# Treinar (reduzido para demonstração)\n",
        "baseline_history = baseline_trainer.train(num_epochs=config.num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukQ0vpS_rzP3"
      },
      "source": [
        "## 9. Experimento 2: BERT + RNN para Features Colaborativas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hgqNT3erzP3",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Treinar modelo com RNN\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENTO 2: BERT + RNN (features colaborativas)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "rnn_model = BERTRNNMovieRecommender(config)\n",
        "\n",
        "rnn_trainer = Trainer(\n",
        "    model=rnn_model,\n",
        "    config=config,\n",
        "    train_loader=train_loader,\n",
        "    eval_loader=eval_loader,\n",
        "    use_multitask=False,\n",
        "    experiment_name=\"exp2_bert_rnn\"\n",
        ")\n",
        "\n",
        "rnn_history = rnn_trainer.train(num_epochs=config.num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld8boIMerzP3"
      },
      "source": [
        "## 10. Experimento 3: Multi-Task Learning com Tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUU7DpZirzP3"
      },
      "outputs": [],
      "source": [
        "# Criar DataLoaders para tags\n",
        "tag_train_loader = DataLoader(\n",
        "    tag_train_dataset,\n",
        "    batch_size=config.tags_batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "tag_eval_loader = DataLoader(\n",
        "    tag_test_dataset,\n",
        "    batch_size=config.tags_batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plrkpj03rzP3"
      },
      "outputs": [],
      "source": [
        "# Treinar modelo com multi-task (BERT baseline + tags)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENTO 3: BERT + Multi-Task (user tags)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "base_model_mt = BERTMovieRecommender(config)\n",
        "multitask_model = MultiTaskWrapper(base_model_mt, config)\n",
        "\n",
        "multitask_trainer = Trainer(\n",
        "    model=multitask_model,\n",
        "    config=config,\n",
        "    train_loader=train_loader,\n",
        "    eval_loader=eval_loader,\n",
        "    tag_train_loader=tag_train_loader,\n",
        "    tag_eval_loader=tag_eval_loader,\n",
        "    use_multitask=True,\n",
        "    experiment_name=\"exp3_multitask\"\n",
        ")\n",
        "\n",
        "multitask_history = multitask_trainer.train(num_epochs=config.num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaClvCTtrzP4"
      },
      "source": [
        "## 11. Experimento 4: BERT + RNN + Multi-Task (Modelo Completo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2AYmZiTrzP4",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Modelo completo: RNN + Multi-task\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENTO 4: BERT + RNN + Multi-Task (modelo completo)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "rnn_base_model = BERTRNNMovieRecommender(config)\n",
        "full_model = MultiTaskWrapper(rnn_base_model, config)\n",
        "\n",
        "full_trainer = Trainer(\n",
        "    model=full_model,\n",
        "    config=config,\n",
        "    train_loader=train_loader,\n",
        "    eval_loader=eval_loader,\n",
        "    tag_train_loader=tag_train_loader,\n",
        "    tag_eval_loader=tag_eval_loader,\n",
        "    use_multitask=True,\n",
        "    experiment_name=\"exp4_full\"\n",
        ")\n",
        "\n",
        "full_history = full_trainer.train(num_epochs=config.num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXJvrCE4rzP4"
      },
      "source": [
        "## 12. Comparação dos Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ty2TGCWrzP4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_results(histories, names):\n",
        "    \"\"\"Plota comparação dos resultados\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "    colors = ['#2ecc71', '#3498db', '#e74c3c', '#9b59b6']\n",
        "\n",
        "    # Train Loss\n",
        "    ax = axes[0, 0]\n",
        "    for hist, name, color in zip(histories, names, colors):\n",
        "        ax.plot(hist['train_loss'], label=name, color=color, linewidth=2)\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.set_title('Training Loss')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Eval Loss\n",
        "    ax = axes[0, 1]\n",
        "    for hist, name, color in zip(histories, names, colors):\n",
        "        ax.plot(hist['eval_loss'], label=name, color=color, linewidth=2)\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.set_title('Evaluation Loss')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # nDCG@10\n",
        "    ax = axes[1, 0]\n",
        "    for hist, name, color in zip(histories, names, colors):\n",
        "        ax.plot(hist['ndcg'], label=name, color=color, linewidth=2)\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('nDCG@10')\n",
        "    ax.set_title('nDCG@10 (métrica principal do artigo)')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Recall@10\n",
        "    ax = axes[1, 1]\n",
        "    for hist, name, color in zip(histories, names, colors):\n",
        "        ax.plot(hist['recall'], label=name, color=color, linewidth=2)\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel('Recall@10')\n",
        "    ax.set_title('Recall@10')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_results.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Plotar resultados\n",
        "plot_results(\n",
        "    [baseline_history, rnn_history, multitask_history, full_history],\n",
        "    ['BERT Baseline', 'BERT + RNN', 'BERT + Multi-Task', 'BERT + RNN + Multi-Task']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lj9NYs_jrzP4"
      },
      "outputs": [],
      "source": [
        "# Tabela de resultados finais\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RESULTADOS FINAIS - Comparação com o Artigo\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "results = {\n",
        "    'Modelo': [\n",
        "        'BERT Baseline',\n",
        "        'BERT + RNN',\n",
        "        'BERT + Multi-Task',\n",
        "        'BERT + RNN + Multi-Task',\n",
        "        '--- Artigo Original ---',\n",
        "        'Artigo: Sem RNN, Sem Tags',\n",
        "        'Artigo: Com RNN, Sem Tags',\n",
        "        'Artigo: Sem RNN, Com Tags',\n",
        "        'Artigo: Com RNN, Com Tags'\n",
        "    ],\n",
        "    'nDCG@10': [\n",
        "        max(baseline_history['ndcg']),\n",
        "        max(rnn_history['ndcg']),\n",
        "        max(multitask_history['ndcg']),\n",
        "        max(full_history['ndcg']),\n",
        "        '-',\n",
        "        0.130,\n",
        "        0.165,\n",
        "        0.138,\n",
        "        0.169\n",
        "    ]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Nota: O artigo reporta nDCG@10 de 0.819 para modelos conversacionais\")\n",
        "print(\"completos (tarefa diferente). Nossa implementação foca na tarefa de\")\n",
        "print(\"recomendação one-shot a partir de queries concatenadas.\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlbxdCJtrzP4"
      },
      "source": [
        "## 13. Inferência e Demonstração"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcnAWh-VrzP4"
      },
      "outputs": [],
      "source": [
        "class MovieRecommenderInference:\n",
        "    \"\"\"Classe para inferência com o modelo treinado\"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer, movie_mapper, device, top_k=10):\n",
        "        self.model = model.to(device)\n",
        "        self.model.eval()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.movie_mapper = movie_mapper\n",
        "        self.device = device\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def recommend(self, query, mentioned_movie_names=None):\n",
        "        \"\"\"\n",
        "        Gera recomendações a partir de uma query.\n",
        "\n",
        "        Args:\n",
        "            query: texto da query do usuário\n",
        "            mentioned_movie_names: lista de nomes de filmes mencionados (opcional)\n",
        "\n",
        "        Returns:\n",
        "            Lista de filmes recomendados com scores\n",
        "        \"\"\"\n",
        "        # Tokenizar\n",
        "        encoding = self.tokenizer(\n",
        "            query,\n",
        "            max_length=512,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].to(self.device)\n",
        "        attention_mask = encoding['attention_mask'].to(self.device)\n",
        "\n",
        "        # Preparar filmes mencionados (placeholder se não disponível)\n",
        "        mentioned_movies = torch.zeros(1, 20, dtype=torch.long, device=self.device)\n",
        "        mentioned_mask = torch.zeros(1, 20, dtype=torch.bool, device=self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                mentioned_movies=mentioned_movies,\n",
        "                mentioned_mask=mentioned_mask\n",
        "            )\n",
        "\n",
        "        # Obter top-k\n",
        "        probs = torch.sigmoid(logits).squeeze(0)\n",
        "        top_scores, top_indices = torch.topk(probs, self.top_k)\n",
        "\n",
        "        recommendations = []\n",
        "        for score, idx in zip(top_scores.cpu().numpy(), top_indices.cpu().numpy()):\n",
        "            movie_id = self.movie_mapper.idx_to_movie_id(idx)\n",
        "            movie_name = self.movie_mapper.movie_names.get(movie_id, f\"Movie {movie_id}\")\n",
        "            recommendations.append({\n",
        "                'movie_id': movie_id,\n",
        "                'name': movie_name,\n",
        "                'score': float(score)\n",
        "            })\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "# Criar inference engine com o melhor modelo\n",
        "inference = MovieRecommenderInference(\n",
        "    model=full_model,\n",
        "    tokenizer=bert_tokenizer,\n",
        "    movie_mapper=movie_mapper,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKPUKA1trzP4"
      },
      "outputs": [],
      "source": [
        "# Demonstração de inferência\n",
        "print(\"=\"*60)\n",
        "print(\"DEMONSTRAÇÃO DE RECOMENDAÇÕES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "test_queries = [\n",
        "    \"I like animations and comedies. I enjoyed Toy Story and Finding Nemo.\",\n",
        "    \"I'm looking for something dramatic and artistic. I love Christopher Nolan films.\",\n",
        "    \"Can you recommend some action movies? I like Marvel superhero films.\",\n",
        "    \"I want to watch something scary for Halloween. Horror movies please!\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"\\n{'─'*60}\")\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"{'─'*60}\")\n",
        "\n",
        "    recommendations = inference.recommend(query)\n",
        "\n",
        "    print(\"\\nTop 5 Recomendações:\")\n",
        "    for i, rec in enumerate(recommendations[:5], 1):\n",
        "        print(f\"  {i}. {rec['name']} (score: {rec['score']:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTK_FbpLrzP5"
      },
      "source": [
        "## 14. Análise de Erros e Limitações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzccv0NhrzP5"
      },
      "outputs": [],
      "source": [
        "# Análise conforme discutido no artigo\n",
        "print(\"=\"*70)\n",
        "print(\"ANÁLISE DE LIMITAÇÕES (conforme artigo)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "analysis = \"\"\"\n",
        "1. TAMANHO DO DATASET\n",
        "   - Treino: {} exemplos\n",
        "   - Teste: {} exemplos\n",
        "   - O artigo menciona ~8008 treino e ~2002 avaliação\n",
        "   - Dataset pequeno leva a overfitting\n",
        "\n",
        "2. QUALIDADE DOS DADOS\n",
        "   - Sentenças concatenadas de diálogos podem não ser significativas\n",
        "   - Exemplo: \"Anything artistic [SEP] What's it about?\" não faz sentido isolado\n",
        "\n",
        "3. COBERTURA DE FILMES\n",
        "   - Total de filmes no mapeamento: {}\n",
        "   - Nem todos têm tags de usuários para multi-task\n",
        "\n",
        "4. COMPARAÇÃO COM ARTIGO\n",
        "   - Artigo reporta nDCG@10 entre 0.130 e 0.169\n",
        "   - Modelos conversacionais completos atingem 0.819\n",
        "   - Nossa tarefa é mais difícil (one-shot vs conversacional)\n",
        "\n",
        "5. MELHORIAS OBSERVADAS\n",
        "   - RNN para features colaborativas: +0.035 nDCG (artigo)\n",
        "   - Multi-task com tags: +0.004-0.008 nDCG (artigo)\n",
        "\"\"\".format(\n",
        "    len(train_data),\n",
        "    len(test_data),\n",
        "    movie_mapper.get_num_movies()\n",
        ")\n",
        "\n",
        "print(analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AX283pfrzP5"
      },
      "source": [
        "## 15. Salvar Modelo Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GFXCD--rzP5"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Salvar modelo completo e configurações\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "class NumpyEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, (np.floating, np.integer)):\n",
        "            return obj.item()\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        elif hasattr(obj, 'item'):  # Para tensores PyTorch\n",
        "            return obj.item()\n",
        "        return super().default(obj)\n",
        "\n",
        "save_path = os.path.join(config.save_dir, 'final_model')\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Salvar pesos do modelo\n",
        "torch.save(full_model.state_dict(), os.path.join(save_path, 'model_weights.pt'))\n",
        "\n",
        "# Salvar configurações\n",
        "config_dict = {k: v for k, v in vars(config).items() if not k.startswith('_')}\n",
        "with open(os.path.join(save_path, 'config.json'), 'w') as f:\n",
        "    json.dump(config_dict, f, indent=2, cls=NumpyEncoder)\n",
        "\n",
        "# Salvar mapeamento de filmes\n",
        "with open(os.path.join(save_path, 'movie_mapping.json'), 'w') as f:\n",
        "    json.dump({\n",
        "        'movie_to_idx': movie_mapper.movie_to_idx,\n",
        "        'movie_names': movie_mapper.movie_names\n",
        "    }, f, indent=2, cls=NumpyEncoder)\n",
        "\n",
        "# Salvar histórico de treinamento\n",
        "with open(os.path.join(save_path, 'training_history.json'), 'w') as f:\n",
        "    json.dump({\n",
        "        'baseline': baseline_history,\n",
        "        'rnn': rnn_history,\n",
        "        'multitask': multitask_history,\n",
        "        'full': full_history\n",
        "    }, f, indent=2, cls=NumpyEncoder)\n",
        "\n",
        "print(f\"Modelo salvo em: {save_path}\")\n",
        "print(\"Arquivos salvos:\")\n",
        "for f in os.listdir(save_path):\n",
        "    print(f\"  - {f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvar log no google drive"
      ],
      "metadata": {
        "id": "DROeRs0f_xy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imprime relatorio com os valores encontrados\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RELATÓRIO FINAL\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "LSg5HuLzJVL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOG0Lo6JrzP5"
      },
      "source": [
        "## 16. Conclusão\n",
        "\n",
        "Esta implementação reproduz os principais experimentos do artigo \"BERT one-shot movie recommender system\" de Trung Nguyen (Stanford CS224N).\n",
        "\n",
        "### Resultados Principais:\n",
        "\n",
        "| Configuração | nDCG@10 (Artigo) | nDCG@10 (Nossa Impl.) |\n",
        "|-------------|------------------|----------------------|\n",
        "| BERT Baseline | 0.130 | Veja resultados acima |\n",
        "| + RNN | 0.165 | Veja resultados acima |\n",
        "| + Multi-Task | 0.138 | Veja resultados acima |\n",
        "| + RNN + Multi-Task | 0.169 | Veja resultados acima |\n",
        "\n",
        "### Insights:\n",
        "1. O RNN para features colaborativas melhora significativamente os resultados\n",
        "2. Multi-task learning com tags oferece ganho marginal\n",
        "3. A combinação de ambas técnicas produz o melhor resultado\n",
        "4. O dataset pequeno e a natureza concatenada dos dados limitam o desempenho\n",
        "\n",
        "### Referências:\n",
        "- Nguyen, T. (2024). BERT one-shot movie recommender system. Stanford CS224N.\n",
        "- Li et al. (2018). Towards deep conversational recommendations. NeurIPS.\n",
        "- Penha & Hauff (2020). What does BERT know about books, movies and music? RecSys."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzP5gJftrzP5"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"IMPLEMENTAÇÃO COMPLETA!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nPara continuar o treinamento com mais épocas, ajuste:\")\n",
        "print(\"  config.num_epochs = 200  # Conforme artigo\")\n",
        "print(\"\\nPara usar o modelo treinado:\")\n",
        "print(\"  inference = MovieRecommenderInference(full_model, tokenizer, movie_mapper, device)\")\n",
        "print(\"  recs = inference.recommend('I like action movies')\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qI3m0bgxTjSy",
        "0mvwZO-SEGIz",
        "Brpc9myKrzP1",
        "7EOQYxyVrzP2"
      ],
      "gpuType": "A100",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}